# å®Œæ•´è®­ç»ƒæµç¨‹å‡½æ•°è°ƒç”¨æ€»ç»“

æœ¬æ–‡æ¡£æ±‡æ€»äº†HumanoidVerseæ¡†æ¶ä¸­ä¸€æ¬¡å®Œæ•´è®­ç»ƒçš„æ‰€æœ‰å…³é”®å‡½æ•°è°ƒç”¨ï¼ŒæŒ‰æ‰§è¡Œé¡ºåºè¯¦ç»†åˆ—å‡ºæ¯ä¸ªæ–‡ä»¶ä¸­çš„å…·ä½“å‡½æ•°ã€‚

## ğŸš€ 1. ç¨‹åºå¯åŠ¨ (train_agent.py)

```python
train_agent.py::main(config)
â”œâ”€â”€ 1. é…ç½®å’Œç¯å¢ƒè®¾ç½®
â”‚   â”œâ”€â”€ simulator_type = config.simulator['_target_'].split('.')[-1]
â”‚   â”œâ”€â”€ è®¾ç½®æ—¥å¿—ç³»ç»Ÿ: logger.add(hydra_log_path)
â”‚   â”œâ”€â”€ åˆå§‹åŒ–WandB: wandb.init() [å¯é€‰]
â”‚   â”œâ”€â”€ è®¾ç½®è®¾å¤‡: device = "cuda:0" if torch.cuda.is_available() else "cpu"
â”‚   â””â”€â”€ pre_process_config(config)
â”œâ”€â”€ 2. å®ä¾‹åŒ–ç»„ä»¶
â”‚   â”œâ”€â”€ env = instantiate(config.env, device=device)
â”‚   â””â”€â”€ algo = instantiate(device=device, env=env, config=config.algo)
â””â”€â”€ 3. å¼€å§‹è®­ç»ƒ
    â”œâ”€â”€ algo.setup()
    â”œâ”€â”€ algo.load(checkpoint) [å¯é€‰]
    â””â”€â”€ algo.learn()  # ğŸ¯ è¿›å…¥ä¸»è®­ç»ƒå¾ªç¯
```

## ğŸŒ 2. ç¯å¢ƒåˆå§‹åŒ– (BaseTask â†’ LeggedRobotBase â†’ MotionTracking)

```python
# BaseTask.__init__()
BaseTask.__init__(config, device)
â”œâ”€â”€ torch._C._jit_set_profiling_mode(False)
â”œâ”€â”€ SimulatorClass = get_class(config.simulator._target_)
â”œâ”€â”€ self.simulator = SimulatorClass(config, device)
â”œâ”€â”€ self.simulator.set_headless(headless)
â”œâ”€â”€ self.simulator.setup()
â”œâ”€â”€ self.simulator.setup_terrain(terrain_mesh_type)
â”œâ”€â”€ self._load_assets()
â”œâ”€â”€ self._get_env_origins()
â”œâ”€â”€ self._create_envs()
â”œâ”€â”€ self.simulator.prepare_sim()
â”œâ”€â”€ self.simulator.setup_viewer() [å¦‚æœéheadless]
â””â”€â”€ self._init_buffers()

# LeggedRobotBase.__init__()
LeggedRobotBase.__init__(config, device)
â”œâ”€â”€ super().__init__(config, device)  # è°ƒç”¨BaseTask
â”œâ”€â”€ self._domain_rand_config()
â”œâ”€â”€ self._prepare_reward_function()
â””â”€â”€ self.history_handler = HistoryHandler(...)

# MotionTracking.__init__()
LeggedRobotMotionTracking.__init__(config, device)
â”œâ”€â”€ super().__init__(config, device)  # è°ƒç”¨LeggedRobotBase
â”œâ”€â”€ self._init_motion_lib()
â”œâ”€â”€ self._init_motion_extend()
â”œâ”€â”€ self._init_tracking_config()
â””â”€â”€ self._init_save_motion()
```

## ğŸ§  3. ç®—æ³•åˆå§‹åŒ– (PPO)

```python
# PPO.__init__()
PPO.__init__(env, config, log_dir, device)
â”œâ”€â”€ self.env = env
â”œâ”€â”€ self.config = config
â”œâ”€â”€ self.device = device
â”œâ”€â”€ self._init_config()
â”œâ”€â”€ åˆå§‹åŒ–ç»Ÿè®¡å˜é‡ (rewbuffer, lenbufferç­‰)
â””â”€â”€ self.env.reset_all()

# PPO.setup()
PPO.setup()
â”œâ”€â”€ self._setup_models_and_optimizer()
â”‚   â”œâ”€â”€ self.actor = PPOActor(obs_dim_dict, config, num_actions, init_noise_std)
â”‚   â”œâ”€â”€ self.critic = PPOCritic(obs_dim_dict, config)
â”‚   â”œâ”€â”€ self.actor_optimizer = Adam(self.actor.parameters(), lr)
â”‚   â””â”€â”€ self.critic_optimizer = Adam(self.critic.parameters(), lr)
â””â”€â”€ self._setup_storage()
    â”œâ”€â”€ self.storage = RolloutStorage(num_envs, num_steps_per_env)
    â”œâ”€â”€ æ³¨å†Œè§‚æµ‹é”®å€¼ (actor_obs, critic_obsç­‰)
    â””â”€â”€ æ³¨å†Œç­–ç•¥é”®å€¼ (actions, rewards, valuesç­‰)
```

## ğŸ¯ 4. ä¸»è®­ç»ƒå¾ªç¯ (PPO.learn)

```python
PPO.learn()
â”œâ”€â”€ åˆå§‹åŒ–è®¾ç½®
â”‚   â”œâ”€â”€ è®¾ç½®éšæœºepisodeé•¿åº¦ [å¯é€‰]
â”‚   â”œâ”€â”€ obs_dict = self.env.reset_all()
â”‚   â”œâ”€â”€ obs_dict[obs_key].to(self.device)
â”‚   â””â”€â”€ self._train_mode()
â””â”€â”€ for it in range(current_iter, tot_iter):  # ä¸»è®­ç»ƒå¾ªç¯
    â”œâ”€â”€ start_time = time.time()
    â”œâ”€â”€ obs_dict = self._rollout_step(obs_dict)     # ğŸ”„ æ•°æ®æ”¶é›†
    â”œâ”€â”€ loss_dict = self._training_step()           # ğŸ“ˆ ç­–ç•¥æ›´æ–°
    â”œâ”€â”€ stop_time = time.time()
    â”œâ”€â”€ self._post_epoch_logging(log_dict)          # ğŸ“Š æ—¥å¿—è®°å½•
    â”œâ”€â”€ self.save() [æŒ‰é—´éš”]                        # ğŸ’¾ ä¿å­˜æ¨¡å‹
    â””â”€â”€ self.ep_infos.clear()
```

## ğŸ”„ 5. æ•°æ®æ”¶é›†é˜¶æ®µ (PPO._rollout_step)

```python
PPO._rollout_step(obs_dict)
â”œâ”€â”€ with torch.inference_mode():
â””â”€â”€ for i in range(self.num_steps_per_env):
    â”œâ”€â”€ ğŸ­ ç­–ç•¥æ¨ç†
    â”‚   â”œâ”€â”€ policy_state_dict = self._actor_rollout_step(obs_dict, {})
    â”‚   â”‚   â”œâ”€â”€ actions = self._actor_act_step(obs_dict)
    â”‚   â”‚   â”‚   â””â”€â”€ self.actor.act(obs_dict["actor_obs"])
    â”‚   â”‚   â”‚       â”œâ”€â”€ self.actor.update_distribution(actor_obs)
    â”‚   â”‚   â”‚       â””â”€â”€ self.distribution.sample()
    â”‚   â”‚   â”œâ”€â”€ action_mean = self.actor.action_mean.detach()
    â”‚   â”‚   â”œâ”€â”€ actions_log_prob = self.actor.get_actions_log_prob(actions)
    â”‚   â”‚   â””â”€â”€ return policy_state_dict
    â”‚   â””â”€â”€ values = self._critic_eval_step(obs_dict)
    â”‚       â””â”€â”€ self.critic.evaluate(obs_dict["critic_obs"])
    â”œâ”€â”€ ğŸ’¾ å­˜å‚¨åˆ°ç¼“å†²åŒº
    â”‚   â”œâ”€â”€ self.storage.update_key(obs_key, obs_dict[obs_key])
    â”‚   â””â”€â”€ self.storage.update_key(key, policy_state_dict[key])
    â”œâ”€â”€ ğŸŒ ç¯å¢ƒäº¤äº’
    â”‚   â””â”€â”€ obs_dict, rewards, dones, infos = self.env.step({"actions": actions})
    â”‚       â””â”€â”€ è°ƒç”¨ç¯å¢ƒstep (è§6.ç¯å¢ƒStep)
    â”œâ”€â”€ ğŸ“Š è®°å½•å’Œå¤„ç†
    â”‚   â”œâ”€â”€ self.storage.update_key('rewards', rewards)
    â”‚   â”œâ”€â”€ self.storage.update_key('dones', dones)
    â”‚   â”œâ”€â”€ self.storage.increment_step()
    â”‚   â””â”€â”€ self._process_env_step(rewards, dones, infos)
    â””â”€â”€ ğŸ“ˆ ç»Ÿè®¡æ›´æ–°
        â”œâ”€â”€ self.ep_infos.append(infos['episode'])
        â”œâ”€â”€ self.cur_reward_sum += rewards
        â””â”€â”€ æ›´æ–°episodeç¼“å†²åŒº
â””â”€â”€ ğŸ§® è®¡ç®—GAE
    â”œâ”€â”€ returns, advantages = self._compute_returns(obs_dict, policy_state_dict)
    â”œâ”€â”€ self.storage.batch_update_data('returns', returns)
    â””â”€â”€ self.storage.batch_update_data('advantages', advantages)
```

## ğŸŒ 6. ç¯å¢ƒStep (LeggedRobotBase.step)

```python
LeggedRobotBase.step(actor_state)
â”œâ”€â”€ actions = actor_state["actions"]
â”œâ”€â”€ self._pre_physics_step(actions)
â”‚   â”œâ”€â”€ self.actions = torch.clip(actions, -clip_limit, clip_limit)
â”‚   â””â”€â”€ æ§åˆ¶å»¶è¿Ÿå¤„ç† [å¦‚æœå¯ç”¨]
â”œâ”€â”€ self._physics_step()
â”‚   â”œâ”€â”€ self.render()
â”‚   â””â”€â”€ for _ in range(control_decimation):
â”‚       â”œâ”€â”€ self._apply_force_in_physics_step()
â”‚       â”‚   â”œâ”€â”€ self.torques = self._compute_torques(self.actions_after_delay)
â”‚       â”‚   â””â”€â”€ self.simulator.apply_torques_at_dof(self.torques)
â”‚       â””â”€â”€ self.simulator.simulate_at_each_physics_step()
â””â”€â”€ self._post_physics_step()
    â”œâ”€â”€ self._refresh_sim_tensors()
    â”œâ”€â”€ self.episode_length_buf += 1
    â”œâ”€â”€ self._pre_compute_observations_callback()
    â”‚   â””â”€â”€ å¯¹äºMotionTracking: è·å–å‚è€ƒåŠ¨ä½œçŠ¶æ€
    â”œâ”€â”€ self._check_termination()
    â”œâ”€â”€ self._compute_reward()
    â”œâ”€â”€ self.reset_envs_idx(env_ids)
    â”‚   â”œâ”€â”€ self._reset_root_states(env_ids)
    â”‚   â””â”€â”€ self._reset_dofs(env_ids)
    â”œâ”€â”€ self._compute_observations()
    â”œâ”€â”€ self._post_compute_observations_callback()
    â””â”€â”€ torch.clip(obs_val, -clip_obs, clip_obs)
```

## ğŸ“ˆ 7. ç­–ç•¥æ›´æ–°é˜¶æ®µ (PPO._training_step)

```python
PPO._training_step()
â”œâ”€â”€ loss_dict = self._init_loss_dict_at_training_step()
â”œâ”€â”€ generator = self.storage.mini_batch_generator(num_mini_batches, num_epochs)
â”œâ”€â”€ for policy_state_dict in generator:
â”‚   â”œâ”€â”€ policy_state_dict[key].to(self.device)
â”‚   â””â”€â”€ loss_dict = self._update_algo_step(policy_state_dict, loss_dict)
â”‚       â””â”€â”€ loss_dict = self._update_ppo(policy_state_dict, loss_dict)
â”‚           â”œâ”€â”€ æå–batchæ•°æ®
â”‚           â”œâ”€â”€ é‡æ–°è®¡ç®—ç­–ç•¥è¾“å‡º
â”‚           â”‚   â”œâ”€â”€ self._actor_act_step(policy_state_dict)
â”‚           â”‚   â”œâ”€â”€ actions_log_prob = self.actor.get_actions_log_prob(actions)
â”‚           â”‚   â”œâ”€â”€ value_batch = self._critic_eval_step(policy_state_dict)
â”‚           â”‚   â””â”€â”€ entropy_batch = self.actor.entropy
â”‚           â”œâ”€â”€ è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´ [å¯é€‰]
â”‚           â”œâ”€â”€ è®¡ç®—PPOæŸå¤±
â”‚           â”‚   â”œâ”€â”€ ratio = exp(new_log_prob - old_log_prob)
â”‚           â”‚   â”œâ”€â”€ surrogate_loss = max(surrogate, surrogate_clipped).mean()
â”‚           â”‚   â”œâ”€â”€ value_loss = (returns - values)Â².mean()
â”‚           â”‚   â””â”€â”€ entropy_loss = entropy.mean()
â”‚           â”œâ”€â”€ åå‘ä¼ æ’­å’Œä¼˜åŒ–
â”‚           â”‚   â”œâ”€â”€ self.actor_optimizer.zero_grad()
â”‚           â”‚   â”œâ”€â”€ self.critic_optimizer.zero_grad()
â”‚           â”‚   â”œâ”€â”€ actor_loss.backward()
â”‚           â”‚   â”œâ”€â”€ critic_loss.backward()
â”‚           â”‚   â”œâ”€â”€ nn.utils.clip_grad_norm_(parameters, max_grad_norm)
â”‚           â”‚   â”œâ”€â”€ self.actor_optimizer.step()
â”‚           â”‚   â””â”€â”€ self.critic_optimizer.step()
â”‚           â””â”€â”€ æ›´æ–°æŸå¤±å­—å…¸
â”œâ”€â”€ è®¡ç®—å¹³å‡æŸå¤±
â””â”€â”€ self.storage.clear()
```

## ğŸ¯ 8. Motion Trackingç‰¹æ®Šå¤„ç†

```python
# Motion Trackingåœ¨ç¯å¢ƒstepä¸­çš„ç‰¹æ®Šå¤„ç†
LeggedRobotMotionTracking._pre_compute_observations_callback()
â”œâ”€â”€ super()._pre_compute_observations_callback()
â”œâ”€â”€ motion_times = (episode_length_buf + 1) * dt + motion_start_times
â”œâ”€â”€ motion_res = self._motion_lib.get_motion_state(motion_ids, motion_times, offset)
â””â”€â”€ å­˜å‚¨å‚è€ƒçŠ¶æ€ (ref_root_pos, ref_root_rot, ref_dof_posç­‰)

# Motion Trackingé‡ç½®
LeggedRobotMotionTracking._reset_root_states(env_ids)
â”œâ”€â”€ motion_res = self._motion_lib.get_motion_state(...)
â”œâ”€â”€ æ·»åŠ åˆå§‹åŒ–å™ªå£°
â””â”€â”€ è®¾ç½®æœºå™¨äººçŠ¶æ€ä¸ºå‚è€ƒçŠ¶æ€+å™ªå£°

LeggedRobotMotionTracking._reset_dofs(env_ids)
â”œâ”€â”€ motion_res = self._motion_lib.get_motion_state(...)
â”œâ”€â”€ æ·»åŠ å…³èŠ‚å™ªå£°
â””â”€â”€ è®¾ç½®å…³èŠ‚çŠ¶æ€
```

## ğŸ”„ 9. å®Œæ•´çš„ä¸€æ¬¡è®­ç»ƒè¿­ä»£æµç¨‹

```
1. PPO.learn() å¼€å§‹ä¸€æ¬¡è¿­ä»£
2. PPO._rollout_step() æ•°æ®æ”¶é›†
   â”œâ”€â”€ for step in num_steps_per_env:
   â”‚   â”œâ”€â”€ Actoræ¨ç† â†’ ç”ŸæˆåŠ¨ä½œ
   â”‚   â”œâ”€â”€ Criticæ¨ç† â†’ ä¼°è®¡ä»·å€¼
   â”‚   â”œâ”€â”€ Environment.step() â†’ ç¯å¢ƒäº¤äº’
   â”‚   â”‚   â”œâ”€â”€ é¢„å¤„ç† â†’ åŠ¨ä½œè£å‰ªã€å»¶è¿Ÿ
   â”‚   â”‚   â”œâ”€â”€ ç‰©ç†ä»¿çœŸ â†’ è®¡ç®—æ‰­çŸ©ã€ä»¿çœŸæ­¥è¿›
   â”‚   â”‚   â””â”€â”€ åå¤„ç† â†’ å¥–åŠ±ã€è§‚æµ‹ã€é‡ç½®
   â”‚   â””â”€â”€ å­˜å‚¨ç»éªŒåˆ°RolloutStorage
   â””â”€â”€ è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
3. PPO._training_step() ç­–ç•¥æ›´æ–°
   â”œâ”€â”€ for mini_batch in generator:
   â”‚   â”œâ”€â”€ é‡æ–°è®¡ç®—ç­–ç•¥è¾“å‡º
   â”‚   â”œâ”€â”€ è®¡ç®—PPOä¸‰ç§æŸå¤±
   â”‚   â””â”€â”€ åå‘ä¼ æ’­å’Œä¼˜åŒ–
   â””â”€â”€ æ¸…ç©ºå­˜å‚¨å™¨
4. æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜
5. è¿›å…¥ä¸‹ä¸€æ¬¡è¿­ä»£
```

è¿™ä¸ªå®Œæ•´çš„å‡½æ•°è°ƒç”¨æ€»ç»“å±•ç¤ºäº†ä»ç¨‹åºå¯åŠ¨åˆ°ä¸€æ¬¡è®­ç»ƒè¿­ä»£å®Œæˆçš„æ‰€æœ‰å…³é”®å‡½æ•°è°ƒç”¨ï¼Œå¸®åŠ©æ‚¨ç†è§£æ•´ä¸ªè®­ç»ƒæµç¨‹çš„æ‰§è¡Œé¡ºåºå’Œå„ç»„ä»¶ä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚
