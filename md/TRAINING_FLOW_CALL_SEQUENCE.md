# HumanoidVerseè®­ç»ƒæµç¨‹å®Œæ•´è°ƒç”¨é¡ºåº

æœ¬æ–‡æ¡£è¯¦ç»†åˆ—å‡ºäº†HumanoidVerseæ¡†æ¶ä¸­ä¸€æ¬¡å®Œæ•´è®­ç»ƒæµç¨‹çš„ä»£ç è°ƒç”¨é¡ºåºï¼Œä»å¯åŠ¨åˆ°ä¸€æ¬¡è®­ç»ƒè¿­ä»£çš„æ‰€æœ‰å‡½æ•°è°ƒç”¨ã€‚

## ğŸš€ 1. ç¨‹åºå¯åŠ¨é˜¶æ®µ

### 1.1 ä¸»å‡½æ•°å…¥å£
```
ğŸ“ humanoidverse/train_agent.py
â”œâ”€â”€ main(config: OmegaConf)                    # Hydraä¸»å‡½æ•°å…¥å£
â”‚   â”œâ”€â”€ æ£€æµ‹ä»¿çœŸå™¨ç±»å‹ (IsaacGym/IsaacSim/Genesis)
â”‚   â”œâ”€â”€ è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
â”‚   â”œâ”€â”€ åˆå§‹åŒ–WandB (å¦‚æœå¯ç”¨)
â”‚   â”œâ”€â”€ è®¾ç½®è®¾å¤‡ (CPU/CUDA)
â”‚   â”œâ”€â”€ pre_process_config(config)             # é¢„å¤„ç†é…ç½®
â”‚   â”‚
â”‚   â”œâ”€â”€ env = instantiate(config.env)          # ğŸŒ å®ä¾‹åŒ–ç¯å¢ƒ
â”‚   â”‚   â””â”€â”€ è°ƒç”¨ç¯å¢ƒæ„é€ å‡½æ•° (è§2.ç¯å¢ƒåˆå§‹åŒ–)
â”‚   â”‚
â”‚   â”œâ”€â”€ algo = instantiate(config.algo)        # ğŸ§  å®ä¾‹åŒ–ç®—æ³•
â”‚   â”‚   â””â”€â”€ è°ƒç”¨ç®—æ³•æ„é€ å‡½æ•° (è§3.ç®—æ³•åˆå§‹åŒ–)
â”‚   â”‚
â”‚   â”œâ”€â”€ algo.setup()                           # è®¾ç½®ç®—æ³•ç»„ä»¶
â”‚   â”œâ”€â”€ algo.load(checkpoint) [å¯é€‰]           # åŠ è½½æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ algo.learn()                           # ğŸ¯ å¼€å§‹è®­ç»ƒ (è§4.è®­ç»ƒå¾ªç¯)
```

## ğŸŒ 2. ç¯å¢ƒåˆå§‹åŒ–é˜¶æ®µ

### 2.1 ç¯å¢ƒç±»ç»§æ‰¿é“¾
```
BaseTask.__init__()
â”œâ”€â”€ åˆ›å»ºä»¿çœŸå™¨å®ä¾‹
â”œâ”€â”€ è®¾ç½®ä»¿çœŸå‚æ•°
â”œâ”€â”€ åŠ è½½æœºå™¨äººèµ„äº§
â”œâ”€â”€ åˆ›å»ºç¯å¢ƒ
â””â”€â”€ åˆå§‹åŒ–ç¼“å†²åŒº
    â†“
LeggedRobotBase.__init__()
â”œâ”€â”€ super().__init__()                         # è°ƒç”¨BaseTaskåˆå§‹åŒ–
â”œâ”€â”€ _domain_rand_config()                      # åŸŸéšæœºåŒ–é…ç½®
â”œâ”€â”€ _prepare_reward_function()                 # å‡†å¤‡å¥–åŠ±å‡½æ•°
â””â”€â”€ åˆå§‹åŒ–å†å²è§‚æµ‹å¤„ç†å™¨
    â†“
LeggedRobotMotionTracking.__init__()
â”œâ”€â”€ super().__init__()                         # è°ƒç”¨LeggedRobotBaseåˆå§‹åŒ–
â”œâ”€â”€ _init_motion_lib()                         # åˆå§‹åŒ–åŠ¨ä½œåº“
â”œâ”€â”€ _init_motion_extend()                      # åˆå§‹åŒ–åŠ¨ä½œæ‰©å±•
â”œâ”€â”€ _init_tracking_config()                    # åˆå§‹åŒ–è¿½è¸ªé…ç½®
â””â”€â”€ _init_save_motion()                        # åˆå§‹åŒ–åŠ¨ä½œä¿å­˜
```

### 2.2 BaseTaskè¯¦ç»†åˆå§‹åŒ–
```
ğŸ“ humanoidverse/envs/base_task/base_task.py
BaseTask.__init__(config, device)
â”œâ”€â”€ è®¾ç½®PyTorch JITä¼˜åŒ–
â”œâ”€â”€ åˆ›å»ºä»¿çœŸå™¨
â”‚   â”œâ”€â”€ SimulatorClass = get_class(config.simulator._target_)
â”‚   â”œâ”€â”€ self.simulator = SimulatorClass(config, device)
â”‚   â”œâ”€â”€ self.simulator.set_headless(headless)
â”‚   â””â”€â”€ self.simulator.setup()
â”œâ”€â”€ è®¾ç½®åŸºç¡€å‚æ•° (dt, num_envs, è§‚æµ‹/åŠ¨ä½œç»´åº¦)
â”œâ”€â”€ self.simulator.setup_terrain()
â”œâ”€â”€ self._load_assets()                        # åŠ è½½æœºå™¨äººèµ„äº§
â”œâ”€â”€ self._get_env_origins()                    # è·å–ç¯å¢ƒåŸç‚¹
â”œâ”€â”€ self._create_envs()                        # åˆ›å»ºç¯å¢ƒ
â”œâ”€â”€ self.simulator.prepare_sim()               # å‡†å¤‡ä»¿çœŸ
â”œâ”€â”€ self.simulator.setup_viewer() [å¦‚æœéheadless]
â””â”€â”€ self._init_buffers()                       # åˆå§‹åŒ–ç¼“å†²åŒº
```

## ğŸ§  3. ç®—æ³•åˆå§‹åŒ–é˜¶æ®µ

### 3.1 PPOç®—æ³•åˆå§‹åŒ–
```
ğŸ“ humanoidverse/agents/ppo/ppo.py
PPO.__init__(env, config, log_dir, device)
â”œâ”€â”€ ä¿å­˜ç¯å¢ƒå’Œé…ç½®å¼•ç”¨
â”œâ”€â”€ åˆ›å»ºTensorBoardå†™å…¥å™¨
â”œâ”€â”€ self._init_config()                        # åˆå§‹åŒ–é…ç½®å‚æ•°
â”œâ”€â”€ åˆå§‹åŒ–ç»Ÿè®¡å˜é‡ (rewbuffer, lenbufferç­‰)
â”œâ”€â”€ åˆ›å»ºè¯„ä¼°å›è°ƒåˆ—è¡¨
â””â”€â”€ self.env.reset_all()                       # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
```

### 3.2 PPO.setup()è¯¦ç»†æµç¨‹
```
PPO.setup()
â”œâ”€â”€ self._setup_models_and_optimizer()
â”‚   â”œâ”€â”€ åˆ›å»ºPPOActorç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ self.actor = PPOActor(obs_dim_dict, config, num_actions, init_noise_std)
â”‚   â”‚   â””â”€â”€ self.actor.to(device)
â”‚   â”œâ”€â”€ åˆ›å»ºPPOCriticç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ self.critic = PPOCritic(obs_dim_dict, config)
â”‚   â”‚   â””â”€â”€ self.critic.to(device)
â”‚   â”œâ”€â”€ åˆ›å»ºActorä¼˜åŒ–å™¨: Adam(actor.parameters(), lr)
â”‚   â””â”€â”€ åˆ›å»ºCriticä¼˜åŒ–å™¨: Adam(critic.parameters(), lr)
â””â”€â”€ self._setup_storage()
    â”œâ”€â”€ åˆ›å»ºRolloutStorage(num_envs, num_steps_per_env)
    â”œâ”€â”€ æ³¨å†Œè§‚æµ‹é”®å€¼ (actor_obs, critic_obsç­‰)
    â””â”€â”€ æ³¨å†Œç­–ç•¥é”®å€¼ (actions, rewards, valuesç­‰)
```

## ğŸ¯ 4. ä¸»è®­ç»ƒå¾ªç¯

### 4.1 PPO.learn()ä¸»å¾ªç¯
```
ğŸ“ humanoidverse/agents/ppo/ppo.py
PPO.learn()
â”œâ”€â”€ éšæœºåˆå§‹åŒ–episodeé•¿åº¦ [å¯é€‰]
â”œâ”€â”€ obs_dict = self.env.reset_all()            # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
â”œâ”€â”€ self._train_mode()                         # è®¾ç½®è®­ç»ƒæ¨¡å¼
â””â”€â”€ for it in range(num_learning_iterations):  # ä¸»è®­ç»ƒå¾ªç¯
    â”œâ”€â”€ obs_dict = self._rollout_step(obs_dict)    # ğŸ”„ æ•°æ®æ”¶é›†é˜¶æ®µ
    â”œâ”€â”€ loss_dict = self._training_step()          # ğŸ“ˆ ç­–ç•¥æ›´æ–°é˜¶æ®µ
    â”œâ”€â”€ è®°å½•æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯
    â””â”€â”€ ä¿å­˜æ¨¡å‹ [æŒ‰é—´éš”]
```

### 4.2 æ•°æ®æ”¶é›†é˜¶æ®µ (_rollout_step)
```
PPO._rollout_step(obs_dict)
â”œâ”€â”€ with torch.inference_mode():               # æ¨ç†æ¨¡å¼ï¼Œä¸è®¡ç®—æ¢¯åº¦
â””â”€â”€ for i in range(num_steps_per_env):         # æ¯ä¸ªç¯å¢ƒæ”¶é›†Næ­¥
    â”œâ”€â”€ ğŸ­ ç­–ç•¥æ¨ç†é˜¶æ®µ
    â”‚   â”œâ”€â”€ policy_state_dict = self._actor_rollout_step(obs_dict, {})
    â”‚   â”‚   â”œâ”€â”€ actions = self._actor_act_step(obs_dict)
    â”‚   â”‚   â”‚   â””â”€â”€ return self.actor.act(obs_dict["actor_obs"])
    â”‚   â”‚   â”‚       â”œâ”€â”€ self.actor.update_distribution(actor_obs)
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ mean = self.actor_module(actor_obs)
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ self.distribution = Normal(mean, std)
    â”‚   â”‚   â”‚       â””â”€â”€ return self.distribution.sample()
    â”‚   â”‚   â”œâ”€â”€ action_mean = self.actor.action_mean.detach()
    â”‚   â”‚   â”œâ”€â”€ action_sigma = self.actor.action_std.detach()
    â”‚   â”‚   â””â”€â”€ actions_log_prob = self.actor.get_actions_log_prob(actions)
    â”‚   â””â”€â”€ values = self._critic_eval_step(obs_dict)
    â”‚       â””â”€â”€ return self.critic.evaluate(obs_dict["critic_obs"])
    â”‚
    â”œâ”€â”€ ğŸ’¾ å­˜å‚¨çŠ¶æ€åˆ°ç¼“å†²åŒº
    â”‚   â”œâ”€â”€ å­˜å‚¨è§‚æµ‹: storage.update_key(obs_key, obs_dict[obs_key])
    â”‚   â””â”€â”€ å­˜å‚¨ç­–ç•¥çŠ¶æ€: storage.update_key(key, policy_state_dict[key])
    â”‚
    â”œâ”€â”€ ğŸŒ ç¯å¢ƒäº¤äº’é˜¶æ®µ
    â”‚   â”œâ”€â”€ actor_state = {"actions": actions}
    â”‚   â””â”€â”€ obs_dict, rewards, dones, infos = self.env.step(actor_state)
    â”‚       â””â”€â”€ è°ƒç”¨ç¯å¢ƒstepå‡½æ•° (è§5.ç¯å¢ƒStepè¯¦ç»†æµç¨‹)
    â”‚
    â”œâ”€â”€ ğŸ“Š è®°å½•å’Œå¤„ç†
    â”‚   â”œâ”€â”€ è½¬ç§»æ•°æ®åˆ°è®¾å¤‡
    â”‚   â”œâ”€â”€ å¤„ç†timeoutå¥–åŠ±
    â”‚   â”œâ”€â”€ storage.update_key('rewards', rewards)
    â”‚   â”œâ”€â”€ storage.update_key('dones', dones)
    â”‚   â”œâ”€â”€ storage.increment_step()
    â”‚   â””â”€â”€ self._process_env_step(rewards, dones, infos)
    â”‚       â”œâ”€â”€ self.actor.reset(dones)
    â”‚       â””â”€â”€ self.critic.reset(dones)
    â”‚
    â””â”€â”€ ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
        â”œâ”€â”€ æ›´æ–°episodeå¥–åŠ±å’Œé•¿åº¦
        â””â”€â”€ è®°å½•å®Œæˆçš„episodeåˆ°ç¼“å†²åŒº
â”‚
â””â”€â”€ ğŸ§® è®¡ç®—GAEå’Œå›æŠ¥
    â”œâ”€â”€ returns, advantages = self._compute_returns(obs_dict, policy_state_dict)
    â”‚   â”œâ”€â”€ last_values = self.critic.evaluate(last_obs_dict["critic_obs"])
    â”‚   â””â”€â”€ for step in reversed(range(num_steps)):
    â”‚       â”œâ”€â”€ delta = rewards[step] + Î³*next_values - values[step]
    â”‚       â”œâ”€â”€ advantage = delta + Î³*Î»*advantage
    â”‚       â””â”€â”€ returns[step] = advantage + values[step]
    â”œâ”€â”€ storage.batch_update_data('returns', returns)
    â””â”€â”€ storage.batch_update_data('advantages', advantages)
```

## ğŸŒ 5. ç¯å¢ƒStepè¯¦ç»†æµç¨‹

### 5.1 ç¯å¢ƒstepä¸»æµç¨‹
```
ğŸ“ humanoidverse/envs/legged_base_task/legged_robot_base.py
LeggedRobotBase.step(actor_state)
â”œâ”€â”€ actions = actor_state["actions"]
â”œâ”€â”€ self._pre_physics_step(actions)            # ğŸ”§ é¢„å¤„ç†é˜¶æ®µ
â”œâ”€â”€ self._physics_step()                       # âš¡ ç‰©ç†ä»¿çœŸé˜¶æ®µ
â”œâ”€â”€ self._post_physics_step()                  # ğŸ”„ åå¤„ç†é˜¶æ®µ
â””â”€â”€ return obs_buf_dict, rew_buf, reset_buf, extras
```

### 5.2 é¢„å¤„ç†é˜¶æ®µ (_pre_physics_step)
```
LeggedRobotBase._pre_physics_step(actions)
â”œâ”€â”€ ğŸ”’ åŠ¨ä½œè£å‰ª
â”‚   â”œâ”€â”€ clip_limit = config.robot.control.action_clip_value
â”‚   â””â”€â”€ self.actions = torch.clip(actions, -clip_limit, clip_limit)
â”œâ”€â”€ ğŸ“Š è®°å½•è£å‰ªç»Ÿè®¡
â””â”€â”€ â±ï¸ æ§åˆ¶å»¶è¿Ÿå¤„ç† [å¦‚æœå¯ç”¨åŸŸéšæœºåŒ–]
    â”œâ”€â”€ self.action_queue[:, 1:] = self.action_queue[:, :-1]
    â”œâ”€â”€ self.action_queue[:, 0] = self.actions
    â””â”€â”€ self.actions_after_delay = self.action_queue[env_ids, delay_idx]
```

### 5.3 ç‰©ç†ä»¿çœŸé˜¶æ®µ (_physics_step)
```
LeggedRobotBase._physics_step()
â”œâ”€â”€ self.render()                              # æ¸²æŸ“ [å¦‚æœå¯ç”¨]
â””â”€â”€ for _ in range(control_decimation):        # æ§åˆ¶æŠ½å–å¾ªç¯
    â”œâ”€â”€ self._apply_force_in_physics_step()    # åº”ç”¨åŠ›/æ‰­çŸ©
    â”‚   â””â”€â”€ self.simulator.set_dof_torque_tensor(torques)
    â””â”€â”€ self.simulator.simulate_at_each_physics_step()  # æ‰§è¡Œç‰©ç†æ­¥
```

### 5.4 åå¤„ç†é˜¶æ®µ (_post_physics_step)
```
LeggedRobotBase._post_physics_step()
â”œâ”€â”€ ğŸ”„ åˆ·æ–°ä»¿çœŸçŠ¶æ€
â”‚   â””â”€â”€ self._refresh_sim_tensors()
â”‚       â””â”€â”€ self.simulator.refresh_sim_tensors()
â”œâ”€â”€ ğŸ“Š æ›´æ–°è®¡æ•°å™¨
â”‚   â”œâ”€â”€ self.episode_length_buf += 1
â”‚   â””â”€â”€ self._update_counters_each_step()
â”œâ”€â”€ ğŸ” é¢„å¤„ç†å›è°ƒ
â”‚   â””â”€â”€ self._pre_compute_observations_callback()
â”‚       â”œâ”€â”€ æ›´æ–°base_quat, rpy
â”‚       â”œâ”€â”€ è®¡ç®—base_lin_vel, base_ang_vel
â”‚       â””â”€â”€ è®¡ç®—projected_gravity
â”œâ”€â”€ ğŸ“‹ ä»»åŠ¡æ›´æ–°
â”‚   â””â”€â”€ self._update_tasks_callback()
â”œâ”€â”€ âŒ æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
â”‚   â””â”€â”€ self._check_termination()
â”‚       â”œâ”€â”€ self._update_reset_buf()
â”‚       â”œâ”€â”€ self._update_timeout_buf()
â”‚       â””â”€â”€ self.reset_buf |= self.time_out_buf
â”œâ”€â”€ ğŸ è®¡ç®—å¥–åŠ±
â”‚   â””â”€â”€ self._compute_reward()
â”‚       â””â”€â”€ for each reward_function:
â”‚           â”œâ”€â”€ rew = reward_function() * scale
â”‚           â”œâ”€â”€ åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç³»æ•° [å¦‚æœå¯ç”¨]
â”‚           â””â”€â”€ self.rew_buf += rew
â”œâ”€â”€ ğŸ”„ é‡ç½®ç¯å¢ƒ
â”‚   â”œâ”€â”€ env_ids = self.reset_buf.nonzero().flatten()
â”‚   â””â”€â”€ self.reset_envs_idx(env_ids)
â”‚       â”œâ”€â”€ self._reset_root_states(env_ids)
â”‚       â”œâ”€â”€ self._reset_dofs(env_ids)
â”‚       â””â”€â”€ å…¶ä»–é‡ç½®æ“ä½œ
â”œâ”€â”€ ğŸ”„ åˆ·æ–°éœ€è¦æ›´æ–°çš„ç¯å¢ƒ
â”‚   â”œâ”€â”€ refresh_env_ids = self.need_to_refresh_envs.nonzero().flatten()
â”‚   â”œâ”€â”€ self.simulator.set_actor_root_state_tensor(refresh_env_ids, states)
â”‚   â””â”€â”€ self.simulator.set_dof_state_tensor(refresh_env_ids, dof_state)
â”œâ”€â”€ ğŸ‘ï¸ è®¡ç®—è§‚æµ‹
â”‚   â””â”€â”€ self._compute_observations()
â”‚       â”œâ”€â”€ åˆå§‹åŒ–è§‚æµ‹å­—å…¸
â”‚       â”œâ”€â”€ è®¾ç½®å™ªå£°è¯¾ç¨‹å­¦ä¹ ç³»æ•°
â”‚       â”œâ”€â”€ for obs_key, obs_config in config.obs.obs_dict.items():
â”‚       â”‚   â””â”€â”€ parse_observation(self, obs_config, obs_buf, scales, noise)
â”‚       â””â”€â”€ å¤„ç†å†å²è§‚æµ‹
â”œâ”€â”€ ğŸ”„ åå¤„ç†å›è°ƒ
â”‚   â””â”€â”€ self._post_compute_observations_callback()
â”‚       â”œâ”€â”€ self.last_actions = self.actions
â”‚       â”œâ”€â”€ self.last_dof_pos = self.simulator.dof_pos
â”‚       â””â”€â”€ self.last_dof_vel = self.simulator.dof_vel
â””â”€â”€ âœ‚ï¸ è£å‰ªè§‚æµ‹
    â””â”€â”€ torch.clip(obs_val, -clip_obs, clip_obs)
```

## ğŸ“ˆ 6. ç­–ç•¥æ›´æ–°é˜¶æ®µ

### 6.1 è®­ç»ƒæ­¥éª¤ (_training_step)
```
PPO._training_step()
â”œâ”€â”€ loss_dict = self._init_loss_dict_at_training_step()
â”œâ”€â”€ generator = storage.mini_batch_generator(num_mini_batches, num_epochs)
â”œâ”€â”€ for policy_state_dict in generator:        # éå†æ‰€æœ‰mini-batch
â”‚   â”œâ”€â”€ å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
â”‚   â””â”€â”€ loss_dict = self._update_algo_step(policy_state_dict, loss_dict)
â”‚       â””â”€â”€ loss_dict = self._update_ppo(policy_state_dict, loss_dict)
â”œâ”€â”€ è®¡ç®—å¹³å‡æŸå¤±
â””â”€â”€ self.storage.clear()                       # æ¸…ç©ºå­˜å‚¨å™¨
```

### 6.2 PPOæ›´æ–° (_update_ppo)
```
PPO._update_ppo(policy_state_dict, loss_dict)
â”œâ”€â”€ ğŸ“¥ æå–batchæ•°æ®
â”‚   â”œâ”€â”€ actions_batch, advantages_batch, returns_batch
â”‚   â”œâ”€â”€ old_actions_log_prob_batch, old_mu_batch, old_sigma_batch
â”‚   â””â”€â”€ target_values_batch
â”œâ”€â”€ ğŸ­ é‡æ–°è®¡ç®—ç­–ç•¥è¾“å‡º
â”‚   â”œâ”€â”€ self._actor_act_step(policy_state_dict)
â”‚   â”‚   â””â”€â”€ self.actor.act(obs_dict["actor_obs"])
â”‚   â”œâ”€â”€ actions_log_prob_batch = self.actor.get_actions_log_prob(actions)
â”‚   â”œâ”€â”€ value_batch = self._critic_eval_step(policy_state_dict)
â”‚   â””â”€â”€ entropy_batch = self.actor.entropy
â”œâ”€â”€ ğŸ“Š è®¡ç®—PPOæŸå¤±
â”‚   â”œâ”€â”€ ğŸ¯ Surrogate Loss
â”‚   â”‚   â”œâ”€â”€ ratio = exp(new_log_prob - old_log_prob)
â”‚   â”‚   â”œâ”€â”€ surrogate = -advantages * ratio
â”‚   â”‚   â”œâ”€â”€ surrogate_clipped = -advantages * clamp(ratio, 1-Îµ, 1+Îµ)
â”‚   â”‚   â””â”€â”€ surrogate_loss = max(surrogate, surrogate_clipped).mean()
â”‚   â”œâ”€â”€ ğŸ’° Value Loss
â”‚   â”‚   â””â”€â”€ value_loss = (returns - values)Â².mean()
â”‚   â””â”€â”€ ğŸ² Entropy Loss
â”‚       â””â”€â”€ entropy_loss = entropy.mean()
â”œâ”€â”€ ğŸ”§ ç»„åˆæŸå¤±
â”‚   â”œâ”€â”€ actor_loss = surrogate_loss - entropy_coef * entropy_loss
â”‚   â””â”€â”€ critic_loss = value_loss_coef * value_loss
â”œâ”€â”€ ğŸ”„ åå‘ä¼ æ’­å’Œä¼˜åŒ–
â”‚   â”œâ”€â”€ self.actor_optimizer.zero_grad()
â”‚   â”œâ”€â”€ self.critic_optimizer.zero_grad()
â”‚   â”œâ”€â”€ actor_loss.backward()
â”‚   â”œâ”€â”€ critic_loss.backward()
â”‚   â”œâ”€â”€ æ¢¯åº¦è£å‰ª
â”‚   â”œâ”€â”€ self.actor_optimizer.step()
â”‚   â””â”€â”€ self.critic_optimizer.step()
â””â”€â”€ ğŸ“Š æ›´æ–°æŸå¤±å­—å…¸
```

## ğŸ”„ 7. Motion Trackingç‰¹æ®Šæµç¨‹

### 7.1 Motion Trackingç¯å¢ƒç‰¹æ®Šå¤„ç†
```
ğŸ“ humanoidverse/envs/motion_tracking/motion_tracking.py
LeggedRobotMotionTracking._pre_compute_observations_callback()
â”œâ”€â”€ super()._pre_compute_observations_callback()  # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
â”œâ”€â”€ è®¡ç®—å½“å‰åŠ¨ä½œæ—¶é—´
â”‚   â””â”€â”€ motion_times = episode_length_buf * dt + motion_start_times
â”œâ”€â”€ ä»åŠ¨ä½œåº“è·å–å‚è€ƒçŠ¶æ€
â”‚   â””â”€â”€ motion_res = self._motion_lib.get_motion_state(motion_ids, motion_times, offset)
â””â”€â”€ å­˜å‚¨å‚è€ƒçŠ¶æ€ç”¨äºå¥–åŠ±è®¡ç®—
```

### 7.2 Motion Trackingé‡ç½®æµç¨‹
```
LeggedRobotMotionTracking._reset_root_states(env_ids)
â”œâ”€â”€ è®¡ç®—å½“å‰åŠ¨ä½œæ—¶é—´
â”œâ”€â”€ ä»åŠ¨ä½œåº“è·å–å‚è€ƒçŠ¶æ€
â”œâ”€â”€ æ·»åŠ åˆå§‹åŒ–å™ªå£°
â”‚   â”œâ”€â”€ root_pos_noise, root_rot_noise
â”‚   â””â”€â”€ root_vel_noise, root_ang_vel_noise
â””â”€â”€ è®¾ç½®æœºå™¨äººçŠ¶æ€ä¸ºå‚è€ƒçŠ¶æ€+å™ªå£°
```

è¿™ä¸ªå®Œæ•´çš„è°ƒç”¨æµç¨‹å±•ç¤ºäº†HumanoidVerseæ¡†æ¶ä»å¯åŠ¨åˆ°ä¸€æ¬¡è®­ç»ƒè¿­ä»£çš„æ‰€æœ‰å…³é”®æ­¥éª¤ï¼Œå¸®åŠ©æ‚¨ç†è§£ä»£ç çš„æ‰§è¡Œé¡ºåºå’Œå„ç»„ä»¶ä¹‹é—´çš„äº¤äº’å…³ç³»ã€‚
