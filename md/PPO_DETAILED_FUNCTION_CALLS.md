# PPO.py è¯¦ç»†å‡½æ•°è°ƒç”¨é¡ºåº

æœ¬æ–‡æ¡£è¯¦ç»†åˆ—å‡ºäº† `humanoidverse/agents/ppo/ppo.py` æ–‡ä»¶ä¸­æ¯ä¸ªå‡½æ•°çš„å…·ä½“è°ƒç”¨é¡ºåºå’Œå†…éƒ¨é€»è¾‘ã€‚

## ğŸ¯ 1. PPO.learn() - ä¸»è®­ç»ƒå¾ªç¯

### 1.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
PPO.learn()
â”œâ”€â”€ 1. åˆå§‹åŒ–è®¾ç½®
â”‚   â”œâ”€â”€ è®¾ç½®éšæœºepisodeé•¿åº¦ [å¯é€‰]
â”‚   â”œâ”€â”€ self.env.reset_all()                    # é‡ç½®æ‰€æœ‰ç¯å¢ƒ
â”‚   â”œâ”€â”€ obs_dict[obs_key].to(self.device)       # è½¬ç§»è§‚æµ‹åˆ°è®¾å¤‡
â”‚   â””â”€â”€ self._train_mode()                      # è®¾ç½®è®­ç»ƒæ¨¡å¼
â”‚
â”œâ”€â”€ 2. ä¸»è®­ç»ƒå¾ªç¯ for it in range(current_iter, tot_iter):
â”‚   â”œâ”€â”€ time.time()                             # è®°å½•å¼€å§‹æ—¶é—´
â”‚   â”œâ”€â”€ obs_dict = self._rollout_step(obs_dict) # ğŸ”„ æ•°æ®æ”¶é›†é˜¶æ®µ
â”‚   â”œâ”€â”€ loss_dict = self._training_step()       # ğŸ“ˆ ç­–ç•¥æ›´æ–°é˜¶æ®µ
â”‚   â”œâ”€â”€ time.time()                             # è®°å½•ç»“æŸæ—¶é—´
â”‚   â”œâ”€â”€ self._post_epoch_logging(log_dict)      # ğŸ“Š æ—¥å¿—è®°å½•
â”‚   â”œâ”€â”€ self.save() [æŒ‰é—´éš”]                    # ğŸ’¾ ä¿å­˜æ¨¡å‹
â”‚   â””â”€â”€ self.ep_infos.clear()                   # æ¸…ç©ºepisodeä¿¡æ¯
â”‚
â””â”€â”€ 3. è®­ç»ƒç»“æŸ
    â”œâ”€â”€ self.current_learning_iteration += num_iterations
    â””â”€â”€ self.save()                             # æœ€ç»ˆä¿å­˜
```

## ğŸ”„ 2. PPO._rollout_step() - æ•°æ®æ”¶é›†é˜¶æ®µ

### 2.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
PPO._rollout_step(obs_dict)
â”œâ”€â”€ with torch.inference_mode():                # æ¨ç†æ¨¡å¼ï¼Œä¸è®¡ç®—æ¢¯åº¦
â””â”€â”€ for i in range(self.num_steps_per_env):     # æ¯ä¸ªç¯å¢ƒæ”¶é›†Næ­¥
    â”œâ”€â”€ ğŸ­ ç­–ç•¥æ¨ç†é˜¶æ®µ
    â”‚   â”œâ”€â”€ policy_state_dict = {}
    â”‚   â”œâ”€â”€ policy_state_dict = self._actor_rollout_step(obs_dict, policy_state_dict)
    â”‚   â”‚   â”œâ”€â”€ actions = self._actor_act_step(obs_dict)
    â”‚   â”‚   â”‚   â””â”€â”€ return self.actor.act(obs_dict["actor_obs"])
    â”‚   â”‚   â”‚       â”œâ”€â”€ self.actor.update_distribution(actor_obs)
    â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ mean = self.actor_module(actor_obs)
    â”‚   â”‚   â”‚       â”‚   â””â”€â”€ self.distribution = Normal(mean, std)
    â”‚   â”‚   â”‚       â””â”€â”€ return self.distribution.sample()
    â”‚   â”‚   â”œâ”€â”€ policy_state_dict["actions"] = actions
    â”‚   â”‚   â”œâ”€â”€ action_mean = self.actor.action_mean.detach()
    â”‚   â”‚   â”œâ”€â”€ action_sigma = self.actor.action_std.detach()
    â”‚   â”‚   â”œâ”€â”€ actions_log_prob = self.actor.get_actions_log_prob(actions).detach()
    â”‚   â”‚   â”œâ”€â”€ policy_state_dict["action_mean"] = action_mean
    â”‚   â”‚   â”œâ”€â”€ policy_state_dict["action_sigma"] = action_sigma
    â”‚   â”‚   â”œâ”€â”€ policy_state_dict["actions_log_prob"] = actions_log_prob
    â”‚   â”‚   â””â”€â”€ return policy_state_dict
    â”‚   â”œâ”€â”€ values = self._critic_eval_step(obs_dict).detach()
    â”‚   â”‚   â””â”€â”€ return self.critic.evaluate(obs_dict["critic_obs"])
    â”‚   â””â”€â”€ policy_state_dict["values"] = values
    â”‚
    â”œâ”€â”€ ğŸ’¾ å­˜å‚¨çŠ¶æ€åˆ°ç¼“å†²åŒº
    â”‚   â”œâ”€â”€ for obs_key in obs_dict.keys():
    â”‚   â”‚   â””â”€â”€ self.storage.update_key(obs_key, obs_dict[obs_key])
    â”‚   â””â”€â”€ for obs_ in policy_state_dict.keys():
    â”‚       â””â”€â”€ self.storage.update_key(obs_, policy_state_dict[obs_])
    â”‚
    â”œâ”€â”€ ğŸŒ ç¯å¢ƒäº¤äº’é˜¶æ®µ
    â”‚   â”œâ”€â”€ actions = policy_state_dict["actions"]
    â”‚   â”œâ”€â”€ actor_state = {"actions": actions}
    â”‚   â”œâ”€â”€ obs_dict, rewards, dones, infos = self.env.step(actor_state)
    â”‚   â”œâ”€â”€ obs_dict[obs_key].to(self.device)    # è½¬ç§»è§‚æµ‹åˆ°è®¾å¤‡
    â”‚   â””â”€â”€ rewards.to(self.device), dones.to(self.device)
    â”‚
    â”œâ”€â”€ ğŸ“Š è®°å½•å’Œå¤„ç†
    â”‚   â”œâ”€â”€ self.episode_env_tensors.add(infos["to_log"])
    â”‚   â”œâ”€â”€ rewards_stored = rewards.clone().unsqueeze(1)
    â”‚   â”œâ”€â”€ å¤„ç†timeoutå¥–åŠ± [å¦‚æœæœ‰time_outs]
    â”‚   â”‚   â””â”€â”€ rewards_stored += Î³ * values * time_outs
    â”‚   â”œâ”€â”€ self.storage.update_key('rewards', rewards_stored)
    â”‚   â”œâ”€â”€ self.storage.update_key('dones', dones.unsqueeze(1))
    â”‚   â”œâ”€â”€ self.storage.increment_step()
    â”‚   â””â”€â”€ self._process_env_step(rewards, dones, infos)
    â”‚       â”œâ”€â”€ self.actor.reset(dones)
    â”‚       â””â”€â”€ self.critic.reset(dones)
    â”‚
    â””â”€â”€ ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
        â”œâ”€â”€ if 'episode' in infos:
        â”‚   â””â”€â”€ self.ep_infos.append(infos['episode'])
        â”œâ”€â”€ self.cur_reward_sum += rewards
        â”œâ”€â”€ self.cur_episode_length += 1
        â”œâ”€â”€ new_ids = (dones > 0).nonzero(as_tuple=False)
        â”œâ”€â”€ self.rewbuffer.extend(self.cur_reward_sum[new_ids])
        â”œâ”€â”€ self.lenbuffer.extend(self.cur_episode_length[new_ids])
        â”œâ”€â”€ self.cur_reward_sum[new_ids] = 0
        â””â”€â”€ self.cur_episode_length[new_ids] = 0
â”‚
â””â”€â”€ ğŸ§® è®¡ç®—GAEå’Œå›æŠ¥
    â”œâ”€â”€ self.stop_time = time.time()
    â”œâ”€â”€ self.collection_time = stop_time - start_time
    â”œâ”€â”€ returns, advantages = self._compute_returns(obs_dict, policy_state_dict)
    â”œâ”€â”€ self.storage.batch_update_data('returns', returns)
    â”œâ”€â”€ self.storage.batch_update_data('advantages', advantages)
    â””â”€â”€ return obs_dict
```

## ğŸ§® 3. PPO._compute_returns() - GAEè®¡ç®—

### 3.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
PPO._compute_returns(last_obs_dict, policy_state_dict)
â”œâ”€â”€ 1. åˆå§‹åŒ–
â”‚   â”œâ”€â”€ last_values = self.critic.evaluate(last_obs_dict["critic_obs"]).detach()
â”‚   â”œâ”€â”€ advantage = 0
â”‚   â”œâ”€â”€ values = policy_state_dict['values']
â”‚   â”œâ”€â”€ dones = policy_state_dict['dones']
â”‚   â”œâ”€â”€ rewards = policy_state_dict['rewards']
â”‚   â”œâ”€â”€ è½¬ç§»æ‰€æœ‰å¼ é‡åˆ°è®¾å¤‡
â”‚   â”œâ”€â”€ returns = torch.zeros_like(values)
â”‚   â””â”€â”€ num_steps = returns.shape[0]
â”‚
â”œâ”€â”€ 2. åå‘è®¡ç®—GAE for step in reversed(range(num_steps)):
â”‚   â”œâ”€â”€ if step == num_steps - 1:
â”‚   â”‚   â””â”€â”€ next_values = last_values        # æœ€åä¸€æ­¥ä½¿ç”¨bootstrap
â”‚   â”œâ”€â”€ else:
â”‚   â”‚   â””â”€â”€ next_values = values[step + 1]   # å…¶ä»–æ­¥ä½¿ç”¨ä¸‹ä¸€æ­¥ä»·å€¼
â”‚   â”œâ”€â”€ next_is_not_terminal = 1.0 - dones[step].float()
â”‚   â”œâ”€â”€ delta = rewards[step] + Î³*next_values - values[step]  # TDè¯¯å·®
â”‚   â”œâ”€â”€ advantage = delta + Î³*Î»*advantage    # GAEä¼˜åŠ¿å‡½æ•°
â”‚   â””â”€â”€ returns[step] = advantage + values[step]  # å›æŠ¥
â”‚
â””â”€â”€ 3. æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
    â”œâ”€â”€ advantages = returns - values
    â”œâ”€â”€ advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    â””â”€â”€ return returns, advantages
```

## ğŸ“ˆ 4. PPO._training_step() - ç­–ç•¥æ›´æ–°é˜¶æ®µ

### 4.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
PPO._training_step()
â”œâ”€â”€ 1. åˆå§‹åŒ–
â”‚   â”œâ”€â”€ loss_dict = self._init_loss_dict_at_training_step()
â”‚   â”‚   â”œâ”€â”€ loss_dict['Value'] = 0
â”‚   â”‚   â”œâ”€â”€ loss_dict['Surrogate'] = 0
â”‚   â”‚   â””â”€â”€ loss_dict['Entropy'] = 0
â”‚   â””â”€â”€ generator = self.storage.mini_batch_generator(num_mini_batches, num_epochs)
â”‚
â”œâ”€â”€ 2. Mini-batchè®­ç»ƒå¾ªç¯ for policy_state_dict in generator:
â”‚   â”œâ”€â”€ for policy_state_key in policy_state_dict.keys():
â”‚   â”‚   â””â”€â”€ policy_state_dict[policy_state_key].to(self.device)
â”‚   â””â”€â”€ loss_dict = self._update_algo_step(policy_state_dict, loss_dict)
â”‚       â””â”€â”€ loss_dict = self._update_ppo(policy_state_dict, loss_dict)
â”‚
â”œâ”€â”€ 3. è®¡ç®—å¹³å‡æŸå¤±
â”‚   â”œâ”€â”€ num_updates = num_learning_epochs * num_mini_batches
â”‚   â””â”€â”€ for key in loss_dict.keys():
â”‚       â””â”€â”€ loss_dict[key] /= num_updates
â”‚
â””â”€â”€ 4. æ¸…ç†
    â”œâ”€â”€ self.storage.clear()
    â””â”€â”€ return loss_dict
```

## ğŸ”§ 5. PPO._update_ppo() - PPOæ ¸å¿ƒæ›´æ–°

### 5.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
PPO._update_ppo(policy_state_dict, loss_dict)
â”œâ”€â”€ 1. æå–batchæ•°æ®
â”‚   â”œâ”€â”€ actions_batch = policy_state_dict['actions']
â”‚   â”œâ”€â”€ target_values_batch = policy_state_dict['values']
â”‚   â”œâ”€â”€ advantages_batch = policy_state_dict['advantages']
â”‚   â”œâ”€â”€ returns_batch = policy_state_dict['returns']
â”‚   â”œâ”€â”€ old_actions_log_prob_batch = policy_state_dict['actions_log_prob']
â”‚   â”œâ”€â”€ old_mu_batch = policy_state_dict['action_mean']
â”‚   â””â”€â”€ old_sigma_batch = policy_state_dict['action_sigma']
â”‚
â”œâ”€â”€ 2. é‡æ–°è®¡ç®—ç­–ç•¥è¾“å‡º
â”‚   â”œâ”€â”€ self._actor_act_step(policy_state_dict)
â”‚   â”‚   â””â”€â”€ return self.actor.act(obs_dict["actor_obs"])
â”‚   â”œâ”€â”€ actions_log_prob_batch = self.actor.get_actions_log_prob(actions_batch)
â”‚   â”œâ”€â”€ value_batch = self._critic_eval_step(policy_state_dict)
â”‚   â”œâ”€â”€ mu_batch = self.actor.action_mean
â”‚   â”œâ”€â”€ sigma_batch = self.actor.action_std
â”‚   â””â”€â”€ entropy_batch = self.actor.entropy
â”‚
â”œâ”€â”€ 3. è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´ [å¦‚æœå¯ç”¨]
â”‚   â”œâ”€â”€ with torch.inference_mode():
â”‚   â”œâ”€â”€ kl = torch.sum(torch.log(Ïƒ_new/Ïƒ_old) + (Ïƒ_oldÂ² + (Î¼_old-Î¼_new)Â²)/(2Ïƒ_newÂ²) - 0.5)
â”‚   â”œâ”€â”€ kl_mean = torch.mean(kl)
â”‚   â”œâ”€â”€ if kl_mean > desired_kl * 2.0:
â”‚   â”‚   â””â”€â”€ learning_rate /= 1.5
â”‚   â”œâ”€â”€ elif kl_mean < desired_kl / 2.0:
â”‚   â”‚   â””â”€â”€ learning_rate *= 1.5
â”‚   â””â”€â”€ æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡
â”‚
â”œâ”€â”€ 4. è®¡ç®—PPOæŸå¤±
â”‚   â”œâ”€â”€ ğŸ¯ Surrogate Loss
â”‚   â”‚   â”œâ”€â”€ ratio = exp(new_log_prob - old_log_prob)
â”‚   â”‚   â”œâ”€â”€ surrogate = -advantages * ratio
â”‚   â”‚   â”œâ”€â”€ surrogate_clipped = -advantages * clamp(ratio, 1-Îµ, 1+Îµ)
â”‚   â”‚   â””â”€â”€ surrogate_loss = max(surrogate, surrogate_clipped).mean()
â”‚   â”œâ”€â”€ ğŸ’° Value Loss
â”‚   â”‚   â”œâ”€â”€ if use_clipped_value_loss:
â”‚   â”‚   â”‚   â”œâ”€â”€ value_clipped = old_values + clamp(new_values - old_values, -Îµ, Îµ)
â”‚   â”‚   â”‚   â”œâ”€â”€ value_losses = (new_values - returns)Â²
â”‚   â”‚   â”‚   â”œâ”€â”€ value_losses_clipped = (value_clipped - returns)Â²
â”‚   â”‚   â”‚   â””â”€â”€ value_loss = max(value_losses, value_losses_clipped).mean()
â”‚   â”‚   â””â”€â”€ else: value_loss = (returns - new_values)Â².mean()
â”‚   â”œâ”€â”€ ğŸ² Entropy Loss
â”‚   â”‚   â””â”€â”€ entropy_loss = entropy_batch.mean()
â”‚   â”œâ”€â”€ actor_loss = surrogate_loss - entropy_coef * entropy_loss
â”‚   â””â”€â”€ critic_loss = value_loss_coef * value_loss
â”‚
â”œâ”€â”€ 5. åå‘ä¼ æ’­å’Œä¼˜åŒ–
â”‚   â”œâ”€â”€ self.actor_optimizer.zero_grad()
â”‚   â”œâ”€â”€ self.critic_optimizer.zero_grad()
â”‚   â”œâ”€â”€ actor_loss.backward()
â”‚   â”œâ”€â”€ critic_loss.backward()
â”‚   â”œâ”€â”€ nn.utils.clip_grad_norm_(self.actor.parameters(), max_grad_norm)
â”‚   â”œâ”€â”€ nn.utils.clip_grad_norm_(self.critic.parameters(), max_grad_norm)
â”‚   â”œâ”€â”€ self.actor_optimizer.step()
â”‚   â””â”€â”€ self.critic_optimizer.step()
â”‚
â””â”€â”€ 6. æ›´æ–°æŸå¤±å­—å…¸
    â”œâ”€â”€ loss_dict['Value'] += value_loss.item()
    â”œâ”€â”€ loss_dict['Surrogate'] += surrogate_loss.item()
    â”œâ”€â”€ loss_dict['Entropy'] += entropy_loss.item()
    â””â”€â”€ return loss_dict
```

## ğŸ”§ 6. è¾…åŠ©å‡½æ•°è°ƒç”¨

### 6.1 å…¶ä»–é‡è¦å‡½æ•°
```python
# Actorç›¸å…³
PPO._actor_act_step(obs_dict)
â””â”€â”€ return self.actor.act(obs_dict["actor_obs"])

# Criticç›¸å…³  
PPO._critic_eval_step(obs_dict)
â””â”€â”€ return self.critic.evaluate(obs_dict["critic_obs"])

# ç¯å¢ƒæ­¥éª¤å¤„ç†
PPO._process_env_step(rewards, dones, infos)
â”œâ”€â”€ self.actor.reset(dones)
â””â”€â”€ self.critic.reset(dones)

# è®­ç»ƒæ¨¡å¼è®¾ç½®
PPO._train_mode()
â”œâ”€â”€ self.actor.train()
â””â”€â”€ self.critic.train()

# æ—¥å¿—è®°å½•
PPO._post_epoch_logging(log_dict)
â”œâ”€â”€ è®¡ç®—FPSå’Œç»Ÿè®¡ä¿¡æ¯
â”œâ”€â”€ self.episode_env_tensors.mean_and_clear()
â”œâ”€â”€ self._logging_to_writer(log_dict, train_log_dict, env_log_dict)
â””â”€â”€ æ‰“å°è®­ç»ƒä¿¡æ¯
```

è¿™ä¸ªè¯¦ç»†çš„å‡½æ•°è°ƒç”¨é¡ºåºå±•ç¤ºäº†PPOç®—æ³•çš„å®Œæ•´æ‰§è¡Œæµç¨‹ï¼Œä»æ•°æ®æ”¶é›†åˆ°ç­–ç•¥æ›´æ–°çš„æ¯ä¸€ä¸ªæ­¥éª¤éƒ½æœ‰æ¸…æ™°çš„è°ƒç”¨é“¾ã€‚

---

# Environment.step() è¯¦ç»†å‡½æ•°è°ƒç”¨é¡ºåº

## ğŸŒ 1. LeggedRobotBase.step() - ç¯å¢ƒä¸»æ­¥éª¤

### 1.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase.step(actor_state)
â”œâ”€â”€ actions = actor_state["actions"]
â”œâ”€â”€ self._pre_physics_step(actions)         # ğŸ”§ é¢„å¤„ç†é˜¶æ®µ
â”œâ”€â”€ self._physics_step()                    # âš¡ ç‰©ç†ä»¿çœŸé˜¶æ®µ
â”œâ”€â”€ self._post_physics_step()               # ğŸ”„ åå¤„ç†é˜¶æ®µ
â””â”€â”€ return self.obs_buf_dict, self.rew_buf, self.reset_buf, self.extras
```

## ğŸ”§ 2. _pre_physics_step() - é¢„å¤„ç†é˜¶æ®µ

### 2.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase._pre_physics_step(actions)
â”œâ”€â”€ 1. åŠ¨ä½œè£å‰ª
â”‚   â”œâ”€â”€ clip_action_limit = self.config.robot.control.action_clip_value
â”‚   â””â”€â”€ self.actions = torch.clip(actions, -clip_action_limit, clip_action_limit)
â”œâ”€â”€ 2. è®°å½•è£å‰ªç»Ÿè®¡
â”‚   â””â”€â”€ self.log_dict["action_clip_frac"] = (actions.abs() == clip_limit).sum() / actions.numel()
â””â”€â”€ 3. æ§åˆ¶å»¶è¿Ÿå¤„ç† [å¦‚æœå¯ç”¨åŸŸéšæœºåŒ–]
    â”œâ”€â”€ if self.config.domain_rand.randomize_ctrl_delay:
    â”œâ”€â”€ self.action_queue[:, 1:] = self.action_queue[:, :-1].clone()  # é˜Ÿåˆ—åç§»
    â”œâ”€â”€ self.action_queue[:, 0] = self.actions.clone()               # æ–°åŠ¨ä½œå…¥é˜Ÿ
    â”œâ”€â”€ self.actions_after_delay = self.action_queue[env_ids, delay_idx].clone()
    â””â”€â”€ else: self.actions_after_delay = self.actions.clone()
```

## âš¡ 3. _physics_step() - ç‰©ç†ä»¿çœŸé˜¶æ®µ

### 3.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase._physics_step()
â”œâ”€â”€ self.render()                           # æ¸²æŸ“ [å¦‚æœå¯ç”¨]
â””â”€â”€ for _ in range(control_decimation):     # æ§åˆ¶æŠ½å–å¾ªç¯
    â”œâ”€â”€ self._apply_force_in_physics_step()
    â”‚   â”œâ”€â”€ self.torques = self._compute_torques(self.actions_after_delay)
    â”‚   â”‚   â”œâ”€â”€ actions_scaled = actions * action_scale
    â”‚   â”‚   â”œâ”€â”€ if control_type == "P":     # ä½ç½®æ§åˆ¶
    â”‚   â”‚   â”‚   â””â”€â”€ torques = kp*(target_pos - current_pos) - kd*current_vel
    â”‚   â”‚   â”œâ”€â”€ elif control_type == "V":   # é€Ÿåº¦æ§åˆ¶
    â”‚   â”‚   â”‚   â””â”€â”€ torques = kp*(target_vel - current_vel) - kd*vel_diff
    â”‚   â”‚   â”œâ”€â”€ elif control_type == "T":   # æ‰­çŸ©æ§åˆ¶
    â”‚   â”‚   â”‚   â””â”€â”€ torques = actions_scaled
    â”‚   â”‚   â”œâ”€â”€ æ·»åŠ éšæœºåŠ›å¹²æ‰° [å¦‚æœå¯ç”¨]
    â”‚   â”‚   â””â”€â”€ torch.clip(torques, -torque_limits, torque_limits)
    â”‚   â””â”€â”€ self.simulator.apply_torques_at_dof(self.torques)
    â””â”€â”€ self.simulator.simulate_at_each_physics_step()
```

## ğŸ”„ 4. _post_physics_step() - åå¤„ç†é˜¶æ®µ

### 4.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase._post_physics_step()
â”œâ”€â”€ 1. åˆ·æ–°ä»¿çœŸçŠ¶æ€
â”‚   â””â”€â”€ self._refresh_sim_tensors()
â”‚       â””â”€â”€ self.simulator.refresh_sim_tensors()
â”œâ”€â”€ 2. æ›´æ–°è®¡æ•°å™¨
â”‚   â”œâ”€â”€ self.episode_length_buf += 1
â”‚   â”œâ”€â”€ self._update_counters_each_step()
â”‚   â””â”€â”€ self.last_episode_length_buf = self.episode_length_buf.clone()
â”œâ”€â”€ 3. é¢„å¤„ç†å›è°ƒ
â”‚   â”œâ”€â”€ self._pre_compute_observations_callback()
â”‚   â”‚   â”œâ”€â”€ self.base_quat[:] = self.simulator.base_quat[:]
â”‚   â”‚   â”œâ”€â”€ self.rpy[:] = get_euler_xyz_in_tensor(self.base_quat)
â”‚   â”‚   â”œâ”€â”€ self.base_lin_vel[:] = quat_rotate_inverse(base_quat, root_states[:, 7:10])
â”‚   â”‚   â”œâ”€â”€ self.base_ang_vel[:] = quat_rotate_inverse(base_quat, root_states[:, 10:13])
â”‚   â”‚   â””â”€â”€ self.projected_gravity[:] = quat_rotate_inverse(base_quat, gravity_vec)
â”‚   â””â”€â”€ self._update_tasks_callback()        # ä»»åŠ¡ç‰¹å®šæ›´æ–°
â”œâ”€â”€ 4. æ£€æŸ¥ç»ˆæ­¢å’Œè®¡ç®—å¥–åŠ±
â”‚   â”œâ”€â”€ self._check_termination()
â”‚   â”‚   â”œâ”€â”€ self.reset_buf[:] = 0
â”‚   â”‚   â”œâ”€â”€ self.time_out_buf[:] = 0
â”‚   â”‚   â”œâ”€â”€ self._update_reset_buf()         # æ›´æ–°é‡ç½®ç¼“å†²åŒº
â”‚   â”‚   â”œâ”€â”€ self._update_timeout_buf()       # æ›´æ–°è¶…æ—¶ç¼“å†²åŒº
â”‚   â”‚   â””â”€â”€ self.reset_buf |= self.time_out_buf
â”‚   â””â”€â”€ self._compute_reward()               # è®¡ç®—å¥–åŠ± (è§5.å¥–åŠ±è®¡ç®—)
â”œâ”€â”€ 5. é‡ç½®ç¯å¢ƒ
â”‚   â”œâ”€â”€ env_ids = self.reset_buf.nonzero(as_tuple=False).flatten()
â”‚   â””â”€â”€ self.reset_envs_idx(env_ids)         # é‡ç½®æŒ‡å®šç¯å¢ƒ (è§6.ç¯å¢ƒé‡ç½®)
â”œâ”€â”€ 6. åˆ·æ–°éœ€è¦æ›´æ–°çš„ç¯å¢ƒ
â”‚   â”œâ”€â”€ refresh_env_ids = self.need_to_refresh_envs.nonzero().flatten()
â”‚   â”œâ”€â”€ if len(refresh_env_ids) > 0:
â”‚   â”‚   â”œâ”€â”€ self.simulator.set_actor_root_state_tensor(refresh_env_ids, all_root_states)
â”‚   â”‚   â”œâ”€â”€ self.simulator.set_dof_state_tensor(refresh_env_ids, dof_state)
â”‚   â”‚   â””â”€â”€ self.need_to_refresh_envs[refresh_env_ids] = False
â”œâ”€â”€ 7. è®¡ç®—è§‚æµ‹
â”‚   â””â”€â”€ self._compute_observations()         # è®¡ç®—è§‚æµ‹ (è§7.è§‚æµ‹è®¡ç®—)
â”œâ”€â”€ 8. åå¤„ç†å›è°ƒ
â”‚   â””â”€â”€ self._post_compute_observations_callback()
â”‚       â”œâ”€â”€ self.last_actions[:] = self.actions[:]
â”‚       â”œâ”€â”€ self.last_dof_pos[:] = self.simulator.dof_pos[:]
â”‚       â”œâ”€â”€ self.last_dof_vel[:] = self.simulator.dof_vel[:]
â”‚       â””â”€â”€ self.last_root_vel[:] = self.simulator.robot_root_states[:, 7:13]
â””â”€â”€ 9. è£å‰ªè§‚æµ‹
    â”œâ”€â”€ clip_obs = self.config.normalization.clip_observations
    â””â”€â”€ for obs_key, obs_val in self.obs_buf_dict.items():
        â””â”€â”€ self.obs_buf_dict[obs_key] = torch.clip(obs_val, -clip_obs, clip_obs)
```

## ğŸ 5. _compute_reward() - å¥–åŠ±è®¡ç®—

### 5.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase._compute_reward()
â”œâ”€â”€ self.rew_buf[:] = 0.                    # é‡ç½®å¥–åŠ±ç¼“å†²åŒº
â”œâ”€â”€ for reward_name, reward_func in self.reward_functions.items():
â”‚   â”œâ”€â”€ rew = reward_func() * reward_scale  # è®¡ç®—å•é¡¹å¥–åŠ±
â”‚   â”œâ”€â”€ åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç³»æ•° [å¦‚æœå¯ç”¨]
â”‚   â”‚   â””â”€â”€ rew *= curriculum_factor
â”‚   â”œâ”€â”€ self.rew_buf += rew                 # ç´¯åŠ åˆ°æ€»å¥–åŠ±
â”‚   â””â”€â”€ self.episode_sums[reward_name] += rew  # è®°å½•episodeç´¯è®¡å¥–åŠ±
â””â”€â”€ åº”ç”¨å¥–åŠ±è¯¾ç¨‹å­¦ä¹  [å¦‚æœå¯ç”¨]
    â””â”€â”€ self.rew_buf *= self.current_reward_curriculum_value
```

## ğŸ”„ 6. reset_envs_idx() - ç¯å¢ƒé‡ç½®

### 6.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase.reset_envs_idx(env_ids)
â”œâ”€â”€ if len(env_ids) == 0: return           # æ— éœ€é‡ç½®åˆ™è¿”å›
â”œâ”€â”€ self._reset_dofs(env_ids)              # é‡ç½®å…³èŠ‚çŠ¶æ€
â”œâ”€â”€ self._reset_root_states(env_ids)       # é‡ç½®æ ¹çŠ¶æ€
â”œâ”€â”€ self._resample_commands(env_ids)       # é‡æ–°é‡‡æ ·å‘½ä»¤
â”œâ”€â”€ é‡ç½®å„ç§ç¼“å†²åŒº
â”‚   â”œâ”€â”€ self.reset_buf[env_ids] = 1
â”‚   â”œâ”€â”€ self.episode_length_buf[env_ids] = 0
â”‚   â”œâ”€â”€ self.episode_sums[reward_name][env_ids] = 0
â”‚   â””â”€â”€ å…¶ä»–ä»»åŠ¡ç‰¹å®šç¼“å†²åŒºé‡ç½®
â”œâ”€â”€ self.extras["episode"] = {}            # è®°å½•episodeä¿¡æ¯
â””â”€â”€ self.need_to_refresh_envs[env_ids] = True  # æ ‡è®°éœ€è¦åˆ·æ–°
```

## ğŸ‘ï¸ 7. _compute_observations() - è§‚æµ‹è®¡ç®—

### 7.1 å‡½æ•°è°ƒç”¨é¡ºåº
```python
LeggedRobotBase._compute_observations()
â”œâ”€â”€ 1. åˆå§‹åŒ–è§‚æµ‹å­—å…¸
â”‚   â”œâ”€â”€ self.obs_buf_dict_raw = {}          # åŸå§‹è§‚æµ‹
â”‚   â””â”€â”€ self.hist_obs_dict = {}             # å†å²è§‚æµ‹
â”œâ”€â”€ 2. è®¾ç½®å™ªå£°è¯¾ç¨‹å­¦ä¹ 
â”‚   â”œâ”€â”€ if self.add_noise_currculum:
â”‚   â”‚   â””â”€â”€ noise_extra_scale = self.current_noise_curriculum_value
â”‚   â””â”€â”€ else: noise_extra_scale = 1.
â”œâ”€â”€ 3. è®¡ç®—å„ç§è§‚æµ‹é¡¹
â”‚   â””â”€â”€ for obs_key, obs_config in self.config.obs.obs_dict.items():
â”‚       â”œâ”€â”€ self.obs_buf_dict_raw[obs_key] = dict()
â”‚       â””â”€â”€ parse_observation(
â”‚           self,                           # ç¯å¢ƒå®ä¾‹
â”‚           obs_config,                     # è§‚æµ‹é…ç½®
â”‚           self.obs_buf_dict_raw[obs_key], # è¾“å‡ºå­—å…¸
â”‚           obs_scales,                     # è§‚æµ‹ç¼©æ”¾
â”‚           noise_scales,                   # å™ªå£°ç¼©æ”¾
â”‚           noise_extra_scale               # é¢å¤–å™ªå£°
â”‚       )
â”œâ”€â”€ 4. è®¡ç®—å†å²è§‚æµ‹
â”‚   â”œâ”€â”€ history_obs_list = self.history_handler.history.keys()
â”‚   â””â”€â”€ parse_observation(self, history_obs_list, self.hist_obs_dict, ...)
â”œâ”€â”€ 5. åå¤„ç†è§‚æµ‹
â”‚   â””â”€â”€ self._post_config_observation_callback()
â”‚       â”œâ”€â”€ self.obs_buf_dict = dict()
â”‚       â””â”€â”€ for obs_key, obs_config in self.config.obs.obs_dict.items():
â”‚           â”œâ”€â”€ obs_keys = sorted(obs_config)
â”‚           â””â”€â”€ self.obs_buf_dict[obs_key] = torch.cat([raw_obs[key] for key in obs_keys])
```

è¿™ä¸ªè¯¦ç»†çš„ç¯å¢ƒstepå‡½æ•°è°ƒç”¨é¡ºåºå±•ç¤ºäº†ä»åŠ¨ä½œè¾“å…¥åˆ°è§‚æµ‹è¾“å‡ºçš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬ç‰©ç†ä»¿çœŸã€å¥–åŠ±è®¡ç®—ã€ç¯å¢ƒé‡ç½®ç­‰æ‰€æœ‰å…³é”®æ­¥éª¤ã€‚
