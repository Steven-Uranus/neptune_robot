# HumanoidVerse è¯¾ç¨‹å¼é‡‡æ ·è¯¦ç»†åˆ†æ

æœ¬æ–‡æ¡£è¯¦ç»†åˆ—å‡ºäº†HumanoidVerseé¡¹ç›®ä¸­æ‰€æœ‰çš„è¯¾ç¨‹å¼é‡‡æ ·(Curriculum Sampling)å’Œè¯¾ç¨‹å­¦ä¹ (Curriculum Learning)æœºåˆ¶ã€‚

## ğŸ¯ 1. è¯¾ç¨‹å­¦ä¹ æ¦‚è¿°

HumanoidVerseæ¡†æ¶å®ç°äº†å¤šç§è¯¾ç¨‹å­¦ä¹ æœºåˆ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´è®­ç»ƒéš¾åº¦æ¥æé«˜å­¦ä¹ æ•ˆç‡å’Œç¨³å®šæ€§ï¼š

### 1.1 è¯¾ç¨‹å­¦ä¹ ç±»å‹
- **å¥–åŠ±è¯¾ç¨‹å­¦ä¹ ** (Reward Curriculum)
- **è§‚æµ‹å™ªå£°è¯¾ç¨‹å­¦ä¹ ** (Observation Noise Curriculum)
- **å¥–åŠ±é™åˆ¶è¯¾ç¨‹å­¦ä¹ ** (Reward Limits Curriculum)
- **ç»ˆæ­¢æ¡ä»¶è¯¾ç¨‹å­¦ä¹ ** (Termination Curriculum)
- **åœ°å½¢è¯¾ç¨‹å­¦ä¹ ** (Terrain Curriculum)
- ~~**åŠ¨ä½œé‡‡æ ·è¯¾ç¨‹å­¦ä¹ ** (Motion Sampling Curriculum)~~ âŒ **ä¸æ˜¯è¯¾ç¨‹å¼**

## ğŸ† 2. å¥–åŠ±è¯¾ç¨‹å­¦ä¹  (Reward Penalty Curriculum)

### 2.1 å®ç°ä½ç½®
```
ğŸ“ humanoidverse/envs/legged_base_task/legged_robot_base.py
â”œâ”€â”€ _update_reward_penalty_curriculum()     # æ›´æ–°å¥–åŠ±æƒ©ç½šè¯¾ç¨‹
â”œâ”€â”€ _compute_reward()                       # åº”ç”¨è¯¾ç¨‹å­¦ä¹ ç³»æ•°
â””â”€â”€ _reset_tasks_callback()                 # åœ¨é‡ç½®æ—¶æ›´æ–°è¯¾ç¨‹
```

### 2.2 æ ¸å¿ƒå®ç°
```python
def _update_reward_penalty_curriculum(self):
    """
    åŸºäºå¹³å‡episodeé•¿åº¦æ›´æ–°å¥–åŠ±æƒ©ç½šè¯¾ç¨‹
    
    é€»è¾‘ï¼š
    - episodeé•¿åº¦çŸ­ â†’ å¢åŠ æƒ©ç½š (é™ä½æƒ©ç½šç³»æ•°)
    - episodeé•¿åº¦é•¿ â†’ å‡å°‘æƒ©ç½š (æé«˜æƒ©ç½šç³»æ•°)
    """
    if self.average_episode_length < self.config.rewards.reward_penalty_level_down_threshold:
        # episodeå¤ªçŸ­ï¼Œå¢åŠ æƒ©ç½šå¼ºåº¦
        self.reward_penalty_scale *= (1 - self.config.rewards.reward_penalty_degree)
    elif self.average_episode_length > self.config.rewards.reward_penalty_level_up_threshold:
        # episodeè¶³å¤Ÿé•¿ï¼Œå‡å°‘æƒ©ç½šå¼ºåº¦
        self.reward_penalty_scale *= (1 + self.config.rewards.reward_penalty_degree)
    
    # é™åˆ¶åœ¨æŒ‡å®šèŒƒå›´å†…
    self.reward_penalty_scale = np.clip(
        self.reward_penalty_scale,
        self.config.rewards.reward_min_penalty_scale,
        self.config.rewards.reward_max_penalty_scale
    )
```

### 2.3 é…ç½®å‚æ•°
```yaml
# humanoidverse/config/rewards/loco/reward_h1_locomotion_10dof.yaml
reward_penalty_curriculum: False
reward_initial_penalty_scale: 1.0
reward_min_penalty_scale: 0.0
reward_max_penalty_scale: 1.0
reward_penalty_level_down_threshold: 400
reward_penalty_level_up_threshold: 700
reward_penalty_degree: 0.00001

# å—å½±å“çš„å¥–åŠ±é¡¹
reward_penalty_reward_names: [
  "penalty_torques",
  "penalty_dof_acc", 
  "penalty_dof_vel",
  "penalty_action_rate",
  "penalty_feet_contact_forces",
  "limits_dof_pos",
  "limits_dof_vel"
]
```

## ğŸ”Š 3. è§‚æµ‹å™ªå£°è¯¾ç¨‹å­¦ä¹  (Observation Noise Curriculum)

### 3.1 å®ç°ä½ç½®
```
ğŸ“ humanoidverse/envs/legged_base_task/legged_robot_base.py
â”œâ”€â”€ _update_obs_noise_curriculum()          # æ›´æ–°è§‚æµ‹å™ªå£°è¯¾ç¨‹
â”œâ”€â”€ _compute_observations()                 # åº”ç”¨å™ªå£°è¯¾ç¨‹ç³»æ•°
â””â”€â”€ _reset_tasks_callback()                 # åœ¨é‡ç½®æ—¶æ›´æ–°è¯¾ç¨‹
```

### 3.2 æ ¸å¿ƒå®ç°
```python
def _update_obs_noise_curriculum(self):
    """
    åŸºäºå¹³å‡episodeé•¿åº¦åŠ¨æ€è°ƒæ•´è§‚æµ‹å™ªå£°æ°´å¹³
    
    é€»è¾‘ï¼š
    - episodeé•¿åº¦çŸ­ â†’ å‡å°‘å™ªå£° (é™ä½éš¾åº¦)
    - episodeé•¿åº¦é•¿ â†’ å¢åŠ å™ªå£° (æé«˜éš¾åº¦)
    """
    if self.average_episode_length < self.config.obs.soft_dof_pos_curriculum_level_down_threshold:
        # episodeå¤ªçŸ­ï¼Œå‡å°‘å™ªå£°
        self.current_noise_curriculum_value *= (1 - self.config.obs.soft_dof_pos_curriculum_degree)
    elif self.average_episode_length > self.config.rewards.reward_penalty_level_up_threshold:
        # episodeè¶³å¤Ÿé•¿ï¼Œå¢åŠ å™ªå£°
        self.current_noise_curriculum_value *= (1 + self.config.obs.soft_dof_pos_curriculum_degree)
    
    # é™åˆ¶å™ªå£°èŒƒå›´
    self.current_noise_curriculum_value = np.clip(
        self.current_noise_curriculum_value,
        self.config.obs.noise_value_min,
        self.config.obs.noise_value_max
    )

def _compute_observations(self):
    # åº”ç”¨å™ªå£°è¯¾ç¨‹å­¦ä¹ ç³»æ•°
    if self.add_noise_currculum:
        noise_extra_scale = self.current_noise_curriculum_value
    else:
        noise_extra_scale = 1.0
    
    # åœ¨è§‚æµ‹è®¡ç®—ä¸­ä½¿ç”¨å™ªå£°ç³»æ•°
    parse_observation(
        self, obs_config, obs_buf,
        obs_scales, noise_scales, 
        noise_extra_scale  # è¯¾ç¨‹å­¦ä¹ çš„é¢å¤–å™ªå£°ç³»æ•°
    )
```

### 3.3 é…ç½®å‚æ•°
```yaml
# humanoidverse/config/obs/loco/leggedloco_obs_singlestep_withlinvel.yaml
add_noise_currculum: False
noise_initial_value: 0.05
noise_value_max: 1.00
noise_value_min: 0.00001
soft_dof_pos_curriculum_degree: 0.00001
soft_dof_pos_curriculum_level_down_threshold: 100
soft_dof_pos_curriculum_level_up_threshold: 900
```

## âš–ï¸ 4. å¥–åŠ±é™åˆ¶è¯¾ç¨‹å­¦ä¹  (Reward Limits Curriculum)

### 4.1 å®ç°ä½ç½®
```
ğŸ“ humanoidverse/envs/legged_base_task/legged_robot_base.py
â”œâ”€â”€ _update_reward_limits_curriculum()      # æ›´æ–°å¥–åŠ±é™åˆ¶è¯¾ç¨‹
â”œâ”€â”€ å„ç§limitså¥–åŠ±å‡½æ•°                      # åº”ç”¨è¯¾ç¨‹å­¦ä¹ çš„é™åˆ¶å€¼
â””â”€â”€ _reset_tasks_callback()                 # åœ¨é‡ç½®æ—¶æ›´æ–°è¯¾ç¨‹
```

### 4.2 æ ¸å¿ƒå®ç°
```python
def _update_reward_limits_curriculum(self):
    """
    åŠ¨æ€è°ƒæ•´å…³èŠ‚ä½ç½®ã€é€Ÿåº¦ã€æ‰­çŸ©çš„è½¯é™åˆ¶
    """
    # å…³èŠ‚ä½ç½®é™åˆ¶è¯¾ç¨‹
    if self.use_reward_limits_dof_pos_curriculum:
        if self.average_episode_length < threshold_down:
            # episodeçŸ­ï¼Œæ”¾å®½é™åˆ¶
            self.soft_dof_pos_curriculum_value *= (1 + degree)
        elif self.average_episode_length > threshold_up:
            # episodeé•¿ï¼Œæ”¶ç´§é™åˆ¶
            self.soft_dof_pos_curriculum_value *= (1 - degree)
        
        self.soft_dof_pos_curriculum_value = np.clip(
            self.soft_dof_pos_curriculum_value,
            min_limit, max_limit
        )
    
    # å…³èŠ‚é€Ÿåº¦é™åˆ¶è¯¾ç¨‹ (ç±»ä¼¼é€»è¾‘)
    if self.use_reward_limits_dof_vel_curriculum:
        # ... ç±»ä¼¼çš„æ›´æ–°é€»è¾‘
    
    # æ‰­çŸ©é™åˆ¶è¯¾ç¨‹ (ç±»ä¼¼é€»è¾‘)  
    if self.use_reward_limits_torque_curriculum:
        # ... ç±»ä¼¼çš„æ›´æ–°é€»è¾‘
```

### 4.3 é…ç½®å‚æ•°
```yaml
# humanoidverse/config/rewards/motion_tracking/reward_motion_tracking_dm_2real.yaml
reward_limits_curriculum:
  soft_dof_pos_curriculum: True
  soft_dof_pos_initial_limit: 1.15
  soft_dof_pos_max_limit: 1.15
  soft_dof_pos_min_limit: 0.95
  soft_dof_pos_curriculum_degree: 0.00000025
  soft_dof_pos_curriculum_level_down_threshold: 40
  soft_dof_pos_curriculum_level_up_threshold: 42

  soft_dof_vel_curriculum: True
  soft_dof_vel_initial_limit: 1.15
  soft_dof_vel_max_limit: 1.25
  soft_dof_vel_min_limit: 0.95
  soft_dof_vel_curriculum_degree: 0.00000025
  soft_dof_vel_curriculum_level_down_threshold: 40
  soft_dof_vel_curriculum_level_up_threshold: 42
```

## âŒ 5. ç»ˆæ­¢æ¡ä»¶è¯¾ç¨‹å­¦ä¹  (Termination Curriculum)

### 5.1 å®ç°ä½ç½® (Motion Tracking)
```
ğŸ“ humanoidverse/envs/motion_tracking/motion_tracking.py
â”œâ”€â”€ _update_terminate_when_motion_far_curriculum()  # æ›´æ–°ç»ˆæ­¢è·ç¦»è¯¾ç¨‹
â”œâ”€â”€ _reset_tasks_callback()                         # åœ¨é‡ç½®æ—¶æ›´æ–°è¯¾ç¨‹
â””â”€â”€ _check_termination()                            # åº”ç”¨åŠ¨æ€ç»ˆæ­¢é˜ˆå€¼
```

### 5.2 æ ¸å¿ƒå®ç°
```python
def _update_terminate_when_motion_far_curriculum(self):
    """
    åŠ¨æ€è°ƒæ•´åŠ¨ä½œåç¦»ç»ˆæ­¢é˜ˆå€¼
    
    é€»è¾‘ï¼š
    - episodeçŸ­ â†’ æ”¾å®½ç»ˆæ­¢æ¡ä»¶ (å¢åŠ é˜ˆå€¼)
    - episodeé•¿ â†’ æ”¶ç´§ç»ˆæ­¢æ¡ä»¶ (å‡å°‘é˜ˆå€¼)
    """
    if self.average_episode_length < threshold_down:
        # episodeå¤ªçŸ­ï¼Œæ”¾å®½ç»ˆæ­¢æ¡ä»¶
        self.terminate_when_motion_far_threshold *= (1 + degree)
    elif self.average_episode_length > threshold_up:
        # episodeè¶³å¤Ÿé•¿ï¼Œæ”¶ç´§ç»ˆæ­¢æ¡ä»¶
        self.terminate_when_motion_far_threshold *= (1 - degree)
    
    # é™åˆ¶é˜ˆå€¼èŒƒå›´
    self.terminate_when_motion_far_threshold = np.clip(
        self.terminate_when_motion_far_threshold,
        threshold_min, threshold_max
    )
```

## ğŸ”ï¸ 6. åœ°å½¢è¯¾ç¨‹å­¦ä¹  (Terrain Curriculum)

### 6.1 å®ç°ä½ç½®
```
ğŸ“ humanoidverse/envs/env_utils/terrain.py
â”œâ”€â”€ curriculum_terrain()                    # ç”Ÿæˆè¯¾ç¨‹åœ°å½¢
â”œâ”€â”€ curiculum()                            # ç®€åŒ–ç‰ˆè¯¾ç¨‹åœ°å½¢
â””â”€â”€ randomized_terrain()                   # éšæœºåœ°å½¢(éè¯¾ç¨‹)
```

### 6.2 æ ¸å¿ƒå®ç°
```python
def curriculum_terrain(self):
    """
    ç”Ÿæˆè¯¾ç¨‹å¼åœ°å½¢ï¼šéš¾åº¦ä»æ˜“åˆ°éš¾æ¸è¿›
    """
    # æŒ‰æ¯”ä¾‹åˆ†é…ä¸åŒç±»å‹çš„åœ°å½¢
    proportions = np.array(self.cfg.terrain_proportions) / np.sum(self.cfg.terrain_proportions)
    
    # ä¸ºæ¯ç§åœ°å½¢ç±»å‹åˆ†é…åˆ—èŒƒå›´
    sub_terrain_dict = {}
    for terrain_type in self.cfg.terrain_types:
        # è®¡ç®—è¯¥åœ°å½¢ç±»å‹çš„åˆ—èŒƒå›´
        start_col, end_col = calculate_col_range(terrain_type, proportions)
        sub_terrain_dict[terrain_type] = (start_col, end_col)
    
    # ç”Ÿæˆè¯¾ç¨‹åœ°å½¢
    for terrain_type, (start_col, end_col) in sub_terrain_dict.items():
        for j in range(start_col, end_col):
            for i in range(self.cfg.num_rows):
                difficulty = i / self.cfg.num_rows  # éš¾åº¦ä»0åˆ°1æ¸è¿›
                terrain = self.make_terrain(terrain_type, difficulty)
                self.add_terrain_to_map(terrain, i, j)

def curiculum(self):
    """
    ç®€åŒ–ç‰ˆè¯¾ç¨‹åœ°å½¢ï¼šè¡Œè¡¨ç¤ºéš¾åº¦ï¼Œåˆ—è¡¨ç¤ºç±»å‹
    """
    for j in range(self.cfg.num_cols):
        for i in range(self.cfg.num_rows):
            difficulty = i / self.cfg.num_rows      # è¡Œï¼šéš¾åº¦æ¸è¿›
            choice = j / self.cfg.num_cols + 0.001  # åˆ—ï¼šç±»å‹å˜åŒ–
            terrain = self.make_terrain(choice, difficulty)
            self.add_terrain_to_map(terrain, i, j)
```

### 6.3 é…ç½®å‚æ•°
```yaml
# humanoidverse/config/terrain/terrain_locomotion.yaml
terrain:
  curriculum: False  # æ˜¯å¦å¯ç”¨è¯¾ç¨‹åœ°å½¢
  num_rows: 10       # åœ°å½¢è¡Œæ•°(éš¾åº¦çº§åˆ«)
  num_cols: 20       # åœ°å½¢åˆ—æ•°(åœ°å½¢ç±»å‹)
  terrain_types: ["flat", "rough", "stairs", "slopes"]
  terrain_proportions: [0.2, 0.3, 0.3, 0.2]
```

## ğŸ­ 7. åŠ¨ä½œé‡‡æ ·æœºåˆ¶ (Motion Sampling) - **éè¯¾ç¨‹å¼**

**é‡è¦è¯´æ˜**: ç»è¿‡ä»”ç»†åˆ†æä»£ç ï¼ŒMotion Samplingå®é™…ä¸Š**ä¸æ˜¯è¯¾ç¨‹å¼é‡‡æ ·**ï¼Œè€Œæ˜¯åŸºäºæƒé‡çš„éšæœºé‡‡æ ·æœºåˆ¶ã€‚

### 7.1 å®é™…å®ç°æœºåˆ¶
```
ğŸ“ humanoidverse/utils/motion_lib/motion_lib_base.py
â”œâ”€â”€ setup_constants()                       # åˆå§‹åŒ–é‡‡æ ·æ¦‚ç‡
â”œâ”€â”€ load_motions()                          # åŸºäºæƒé‡çš„éšæœºé‡‡æ ·
â””â”€â”€ _sampling_prob                          # åŠ¨ä½œé‡‡æ ·æƒé‡(ç›®å‰ä¸ºå‡åŒ€åˆ†å¸ƒ)
```

### 7.2 çœŸå®çš„é‡‡æ ·é€»è¾‘
```python
def setup_constants(self):
    """
    åˆå§‹åŒ–åŠ¨ä½œé‡‡æ ·ç›¸å…³çš„å¸¸é‡
    """
    # ç»ˆæ­¢å†å²è®°å½•(é¢„ç•™ç”¨äºæœªæ¥çš„è¯¾ç¨‹å­¦ä¹ )
    self._termination_history = torch.zeros(self._num_unique_motions).to(self._device)
    self._success_rate = torch.zeros(self._num_unique_motions).to(self._device)
    self._sampling_history = torch.zeros(self._num_unique_motions).to(self._device)

    # é‡‡æ ·æ¦‚ç‡ - ç›®å‰ä¸ºå‡åŒ€åˆ†å¸ƒ
    self._sampling_prob = torch.ones(self._num_unique_motions).to(self._device) / self._num_unique_motions

def load_motions(self, random_sample=True, start_idx=0, ...):
    """
    åŠ è½½åŠ¨ä½œçš„æ ¸å¿ƒé€»è¾‘
    """
    if random_sample:
        # åŸºäºæƒé‡çš„å¤šé¡¹å¼é‡‡æ · - ä½†æƒé‡ç›®å‰æ˜¯å‡åŒ€çš„
        sample_idxes = torch.multinomial(
            self._sampling_prob,
            num_samples=num_motion_to_load,
            replacement=True
        ).to(self._device)
    else:
        # é¡ºåºé‡‡æ ·(è¯„ä¼°æ¨¡å¼)
        sample_idxes = torch.remainder(
            torch.arange(num_motion_to_load) + start_idx,
            self._num_unique_motions
        ).to(self._device)
```

### 7.3 å½“å‰çŠ¶æ€åˆ†æ
```python
# å½“å‰çš„é‡‡æ ·æƒé‡æ˜¯å‡åŒ€åˆ†å¸ƒ
self._sampling_prob = torch.ones(num_motions) / num_motions

# è¿™æ„å‘³ç€æ¯ä¸ªåŠ¨ä½œè¢«é€‰ä¸­çš„æ¦‚ç‡ç›¸ç­‰
# å¹¶æ²¡æœ‰åŸºäºå¤æ‚åº¦ã€æˆåŠŸç‡æˆ–å…¶ä»–æŒ‡æ ‡çš„æƒé‡è°ƒæ•´
```

### 7.4 é¢„ç•™çš„è¯¾ç¨‹å­¦ä¹ åŸºç¡€è®¾æ–½
è™½ç„¶å½“å‰ä¸æ˜¯è¯¾ç¨‹å¼é‡‡æ ·ï¼Œä½†ä»£ç ä¸­é¢„ç•™äº†ç›¸å…³åŸºç¡€è®¾æ–½ï¼š

```python
# è¿™äº›å˜é‡ç›®å‰æœªè¢«ä½¿ç”¨ï¼Œä½†ä¸ºæœªæ¥çš„è¯¾ç¨‹å­¦ä¹ åšäº†å‡†å¤‡
self._termination_history    # è®°å½•æ¯ä¸ªåŠ¨ä½œçš„ç»ˆæ­¢å†å²
self._success_rate          # è®°å½•æ¯ä¸ªåŠ¨ä½œçš„æˆåŠŸç‡
self._sampling_history      # è®°å½•æ¯ä¸ªåŠ¨ä½œçš„é‡‡æ ·å†å²
```

### 7.5 å®é™…çš„"å¤šæ ·æ€§"æœºåˆ¶
```python
def _update_tasks_callback(self):
    """
    å®šæœŸé‡æ–°é‡‡æ ·åŠ¨ä½œï¼Œå¢åŠ è®­ç»ƒå¤šæ ·æ€§
    è¿™ä¸æ˜¯è¯¾ç¨‹å¼çš„ï¼Œè€Œæ˜¯ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆç‰¹å®šåŠ¨ä½œ
    """
    if self.config.resample_motion_when_training:
        if self.common_step_counter % self.resample_time_interval == 0:
            self.resample_motion()  # é‡æ–°éšæœºé‡‡æ ·æ‰€æœ‰åŠ¨ä½œ
```

### 7.6 æ€»ç»“
- **å½“å‰å®ç°**: å‡åŒ€éšæœºé‡‡æ ·ï¼Œä¸æ˜¯è¯¾ç¨‹å¼
- **è®¾è®¡æ„å›¾**: å¢åŠ è®­ç»ƒå¤šæ ·æ€§ï¼Œé¿å…è¿‡æ‹Ÿåˆ
- **æœªæ¥æ½œåŠ›**: ä»£ç ç»“æ„æ”¯æŒæ·»åŠ åŸºäºæ€§èƒ½çš„æƒé‡è°ƒæ•´
- **å¯èƒ½æ‰©å±•**: å¯ä»¥åŸºäº`_success_rate`æˆ–`_termination_history`å®ç°çœŸæ­£çš„è¯¾ç¨‹å¼é‡‡æ ·

## ğŸ“Š 8. è¯¾ç¨‹å­¦ä¹ è°ƒåº¦æœºåˆ¶

### 8.1 æ›´æ–°è§¦å‘æ—¶æœº
```python
# åœ¨ç¯å¢ƒé‡ç½®æ—¶æ›´æ–°æ‰€æœ‰è¯¾ç¨‹
def _reset_tasks_callback(self, env_ids):
    self._episodic_domain_randomization(env_ids)
    
    # æ›´æ–°å„ç§è¯¾ç¨‹å­¦ä¹ 
    if self.use_reward_penalty_curriculum:
        self._update_reward_penalty_curriculum()
    
    if self.use_reward_limits_curriculum:
        self._update_reward_limits_curriculum()
    
    if self.add_noise_currculum:
        self._update_obs_noise_curriculum()
    
    # Motion Trackingç‰¹æœ‰
    if self.config.termination_curriculum.terminate_when_motion_far_curriculum:
        self._update_terminate_when_motion_far_curriculum()
```

### 8.2 è¯¾ç¨‹å­¦ä¹ æŒ‡æ ‡
æ‰€æœ‰è¯¾ç¨‹å­¦ä¹ éƒ½åŸºäº **å¹³å‡episodeé•¿åº¦** ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼š
- **episodeé•¿åº¦çŸ­** â†’ æ€§èƒ½å·® â†’ é™ä½éš¾åº¦
- **episodeé•¿åº¦é•¿** â†’ æ€§èƒ½å¥½ â†’ æé«˜éš¾åº¦

### 8.3 è¯¾ç¨‹å­¦ä¹ çš„é€šç”¨æ¨¡å¼
```python
# é€šç”¨è¯¾ç¨‹æ›´æ–°æ¨¡å¼
if average_episode_length < level_down_threshold:
    curriculum_value *= (1 Â± curriculum_degree)  # é™ä½éš¾åº¦
elif average_episode_length > level_up_threshold:
    curriculum_value *= (1 Â± curriculum_degree)  # æé«˜éš¾åº¦

curriculum_value = np.clip(curriculum_value, min_limit, max_limit)
```

## ğŸ¯ 9. è¯¾ç¨‹å­¦ä¹ çš„åº”ç”¨æ•ˆæœ

### 9.1 è®­ç»ƒç¨³å®šæ€§
- **æ¸è¿›å¼éš¾åº¦**ï¼šé¿å…è®­ç»ƒåˆæœŸè¿‡éš¾å¯¼è‡´çš„å­¦ä¹ å¤±è´¥
- **è‡ªé€‚åº”è°ƒæ•´**ï¼šæ ¹æ®æ€§èƒ½åŠ¨æ€è°ƒæ•´éš¾åº¦
- **å¹³æ»‘è¿‡æ¸¡**ï¼šé¿å…éš¾åº¦çªå˜å¯¼è‡´çš„æ€§èƒ½æ³¢åŠ¨

### 9.2 Sim2Realæ•ˆæœ
- **å™ªå£°è¯¾ç¨‹**ï¼šé€æ­¥å¢åŠ è§‚æµ‹å™ªå£°ï¼Œæé«˜é²æ£’æ€§
- **é™åˆ¶è¯¾ç¨‹**ï¼šåŠ¨æ€è°ƒæ•´å®‰å…¨é™åˆ¶ï¼Œå¹³è¡¡æ€§èƒ½å’Œå®‰å…¨
- **åœ°å½¢è¯¾ç¨‹**ï¼šä»ç®€å•åˆ°å¤æ‚åœ°å½¢ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

è¿™ä¸ªè¯¦ç»†çš„è¯¾ç¨‹å¼é‡‡æ ·åˆ†æå±•ç¤ºäº†HumanoidVerseæ¡†æ¶ä¸­ä¸°å¯Œçš„è¯¾ç¨‹å­¦ä¹ æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶å…±åŒä½œç”¨æ¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½ã€‚
